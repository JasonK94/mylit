#' Add Module Scores for a List of Feature Sets
#'
#' This function takes a Seurat object and a list of feature (gene) sets.
#' For each feature set, it calculates a module score using Seurat's AddModuleScore.
#' The name of the resulting metadata column will be the name of the feature set if provided,
#' or a concatenation of gene names joined by "+" if the feature set is unnamed.
#'
#' @param seurat_object A Seurat object.
#' @param feature_sets A named or unnamed list of character vectors. Each character vector is a set of gene names.
#'                     If the list is named, the names will be used for the output module score columns.
#'                     If a list element is unnamed, its name will be generated by concatenating gene names with "+".
#' @param assay Name of the assay to use. Default is the current default assay.
#' @param layer layer to pull expression data from. Default is "data".
#' @param nbin Number of bins for AddModuleScore. Default is 24.
#' @param ctrl Number of control features for AddModuleScore. Default is 100.
#' @param seed Random seed for AddModuleScore. Default is 1.
#' @param search Passed to Seurat::`[.Assay`. Default is FALSE. Relevant if features are not present in the object.
#' @param ... Additional arguments passed to Seurat::AddModuleScore.
#'
#' @return A Seurat object with added module scores in the metadata.
#'
#' @examples
#' \dontrun{
#' # Assuming 'pbmc' is a Seurat object with an RNA assay
#' # Example feature sets
#' gene_sets <- list(
#'   Tcell_activation = c("CD69", "IFNG", "TNF"),
#'   Monocyte_markers = c("CD14", "LYZ"),
#'   c("CCR7", "SELL", "LEF1"), # Unnamed set
#'   Bcell_core = c("MS4A1", "CD19", "CD79A")
#' )
#'
#' pbmc <- AddMultipleModuleScores(seurat_object = pbmc, feature_sets = gene_sets)
#'
#' # Check the new metadata columns
#' head(pbmc@meta.data)
#'
#' # Visualize a score
#' FeaturePlot(pbmc, features = "Tcell_activation1") # Name is appended with a number
#' FeaturePlot(pbmc, features = "CCR7_SELL_LEF11")
#' }
AddMultipleModuleScores <- function(seurat_object,
                                    feature_sets,
                                    assay = NULL,
                                    layer = "data",
                                    nbin = 24,
                                    ctrl = 100,
                                    seed = 1,
                                    search = FALSE,
                                    ...) {
  
  if (!requireNamespace("Seurat", quietly = TRUE))
    stop("Package 'Seurat' is required but not installed.")
  if (!is.list(feature_sets))
    stop("'feature_sets' must be a list of character vectors.")
  
  if (is.null(assay))
    assay <- Seurat::DefaultAssay(seurat_object)
  
  all_genes <- rownames(Seurat::GetAssayData(seurat_object, assay = assay, layer = layer))
  added_cols <- character(0)              # <- 여기에 최종 컬럼명 모음
  
  for (i in seq_along(feature_sets)) {
    
    genes_raw <- feature_sets[[i]]
    genes_use <- base::intersect(genes_raw, all_genes)
    if (length(genes_use) == 0) {
      warning(sprintf("feature set %d: no genes found – skipped", i))
      next
    }
    
    set_name <- names(feature_sets)[i]
    if (is.null(set_name) || set_name == "")
      set_name <- paste(gsub("-", "_", genes_use), collapse = "+")
    
    #— 1) AddModuleScore 실행
    before_cols <- colnames(seurat_object[[]])
    seurat_object <- Seurat::AddModuleScore(
      object   = seurat_object,
      features = list(genes_use),
      name     = set_name,                # AddModuleScore가 set_name1 을 만듦
      assay    = assay,
      layer     = layer,
      nbin     = nbin,
      ctrl     = ctrl,
      seed     = seed,
      search   = search,
      ...
    )
    after_cols  <- colnames(seurat_object[[]])
    new_col     <- base::setdiff(after_cols, before_cols)  # 방금 생긴 컬럼
    
    #— 2) 뒤에 붙은 숫자 제거 & rename
    tidy_col    <- sub("1$", "", new_col)            # 끝의 1 지우기
    if (tidy_col != new_col) {
      colnames(seurat_object[[]])[match(new_col, after_cols)] <- tidy_col
      message(sprintf("renamed '%s' -> '%s'", new_col, tidy_col))
    }
    
    added_cols <- c(added_cols, tidy_col)
  }
  
  #— 3) FeaturePlot 에 바로 써먹을 수 있게 출력
  if (length(added_cols)) {
    msg1 <- "# Copy-&-paste for FeaturePlot:"
    msg2 <- sprintf("FeaturePlot(obj, features = c(%s))",
                    paste(sprintf("'%s'", added_cols), collapse=", "))
    message("\n", msg1, "\n", msg2, "\n")
    flush.console()
  } else {
    warning("No module scores were added.")
  }
  
  
  invisible(seurat_object)
}



#' Visualize Module Scores as Heatmap with Statistical Testing
#'
#' This function uses AddModuleScore results to create a heatmap visualization
#' with z-score normalization and statistical significance indicators
#'
#' @param sobj Seurat object
#' @param gene_sets Named list of gene sets
#' @param group Grouping variable (default: "seurat_clusters")
#' @param assay Assay to use (default: "SCT")
#' @param test_method Statistical test method ("wilcox" or "t")
#' @param p_adjust Method for p-value adjustment (default: "bonferroni")
#' @param show_pval Whether to show p-values on the heatmap
#' @param scale_method How to scale the data: "feature" (scale each feature across groups) or "group" (scale within each group)
#' @param color_limits Manual color scale limits (e.g., c(-2, 2)). If NULL, uses symmetric limits based on data
#' @param ... Additional arguments passed to AddModuleScore
#'
#' @return A list containing the heatmap plot and statistical results
#'
PlotModuleScoreHeatmap <- function(
    sobj,
    gene_sets,
    group = "seurat_clusters",
    assay = "SCT",
    test_method = "wilcox",
    p_adjust = "bonferroni",
    show_pval = TRUE,
    scale_method = "feature",
    color_limits = NULL,
    title = "Module Score Expression per Cluster",
    x_label = "Cluster",
    y_label = "Gene Set",
    ...
) {
  library(Seurat)
  library(dplyr)
  library(tidyr)
  library(ggplot2)
  library(patchwork)
  
  # Validate inputs
  if(is.null(gene_sets) || length(gene_sets) == 0) {
    stop("gene_sets must be provided")
  }
  
  # Ensure gene_sets is a named list
  if(!is.list(gene_sets)) {
    gene_sets <- list(GeneSet1 = gene_sets)
  }
  
  if(is.null(names(gene_sets)) || any(names(gene_sets) == "")) {
    for(i in seq_along(gene_sets)) {
      if(is.null(names(gene_sets)[i]) || names(gene_sets)[i] == "") {
        names(gene_sets)[i] <- paste0("GeneSet", i)
      }
    }
  }
  
  # Add module scores for each gene set
  module_names <- character()
  for(i in seq_along(gene_sets)) {
    set_name <- names(gene_sets)[i]
    
    # Check if genes exist in the dataset
    genes_present <- gene_sets[[i]][gene_sets[[i]] %in% rownames(sobj[[assay]])]
    if(length(genes_present) == 0) {
      warning(paste("No genes from", set_name, "found in the dataset."))
      next
    }
    
    # AddModuleScore adds a number suffix, we'll track it
    before_cols <- colnames(sobj@meta.data)
    sobj <- AddModuleScore(
      object = sobj,
      features = list(genes_present),
      name = set_name,
      assay = assay,
      ...
    )
    after_cols <- colnames(sobj@meta.data)
    new_col <- base::setdiff(after_cols, before_cols)
    
    # Remove the "1" suffix if desired
    if(length(new_col) > 0) {
      clean_name <- sub("1$", "", new_col[1])
      colnames(sobj@meta.data)[colnames(sobj@meta.data) == new_col[1]] <- clean_name
      module_names <- c(module_names, clean_name)
    }
  }
  
  if(length(module_names) == 0) {
    stop("No valid module scores could be calculated")
  }
  
  # Calculate average module scores per group
  Idents(sobj) <- group
  group_levels <- levels(Idents(sobj))
  if(is.null(group_levels)) {
    group_levels <- base::unique(Idents(sobj))
  }
  
  # Extract module score data
  module_data <- sobj@meta.data[, c(group, module_names), drop = FALSE]
  
  # Calculate mean scores per group
  mean_scores <- module_data %>%
    group_by(!!sym(group)) %>%
    summarise(across(all_of(module_names), mean, na.rm = TRUE)) %>%
    as.data.frame()
  
  # Z-score normalization based on scale_method
  z_scores <- mean_scores
  
  if(scale_method == "feature") {
    # Scale each module across all clusters (recommended)
    # 각 module에 대해 모든 cluster의 평균과 표준편차로 정규화
    for(module in module_names) {
      z_scores[[module]] <- scale(mean_scores[[module]])[, 1]
    }
  } else if(scale_method == "group") {
    # Scale all modules within each cluster (original behavior)
    # 각 cluster 내에서 모든 module을 정규화
    z_scores[, module_names] <- t(scale(t(mean_scores[, module_names])))
  } else {
    stop("scale_method must be either 'feature' or 'group'")
  }
  
  # Statistical testing between clusters
  p_values <- matrix(NA, nrow = length(module_names), ncol = length(group_levels),
                     dimnames = list(module_names, group_levels))
  
  for(module in module_names) {
    for(cluster in group_levels) {
      cluster_scores <- module_data[module_data[[group]] == cluster, module]
      other_scores <- module_data[module_data[[group]] != cluster, module]
      
      if(length(cluster_scores) > 2 && length(other_scores) > 2) {
        if(test_method == "wilcox") {
          test_result <- wilcox.test(cluster_scores, other_scores)
        } else if(test_method == "t") {
          test_result <- t.test(cluster_scores, other_scores)
        }
        p_values[module, as.character(cluster)] <- test_result$p.value
      }
    }
  }
  
  # Adjust p-values
  p_adj <- matrix(p.adjust(as.vector(p_values), method = p_adjust),
                  nrow = nrow(p_values), ncol = ncol(p_values),
                  dimnames = dimnames(p_values))
  
  # Prepare data for plotting
  plot_data <- z_scores %>%
    pivot_longer(cols = all_of(module_names),
                 names_to = "Module",
                 values_to = "ZScore")
  
  # Add significance indicators
  sig_data <- as.data.frame(p_adj) %>%
    mutate(Module = rownames(.)) %>%
    pivot_longer(cols = -Module,
                 names_to = group,
                 values_to = "p_adj")
  
  plot_data <- plot_data %>%
    left_join(sig_data, by = c("Module", group))
  
  # Add significance symbols
  plot_data <- plot_data %>%
    mutate(sig_symbol = case_when(
      p_adj < 0.001 ~ "***",
      p_adj < 0.01 ~ "**",
      p_adj < 0.05 ~ "*",
      TRUE ~ ""
    ))
  
  # Sort clusters numerically if possible
  numeric_test <- suppressWarnings(as.numeric(as.character(plot_data[[group]])))
  if(!all(is.na(numeric_test))) {
    plot_data[[group]] <- factor(plot_data[[group]],
                                 levels = as.character(sort(base::unique(numeric_test))))
  }
  
  # Create heatmap with adjusted color scale
  if(is.null(color_limits)) {
    # Use symmetric limits based on max absolute value
    max_abs <- max(abs(plot_data$ZScore), na.rm = TRUE)
    color_limits <- c(-max_abs, max_abs)
  }
  
  p <- ggplot(plot_data, aes_string(x = group, y = "Module", fill = "ZScore")) +
    geom_tile() +
    scale_fill_gradient2(low = "blue", mid = "white", high = "red", 
                         midpoint = 0, limits = color_limits, name = "Z-Score") +
    theme_minimal() +
    labs(title = title, x = x_label, y = y_label) +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
      axis.text.y = element_text(size = 12),
      axis.title = element_text(size = 14, face = "bold"),
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5)
    )
  
  # Add significance indicators if requested
  if(show_pval) {
    p <- p + geom_text(aes(label = sig_symbol), 
                       color = "black", size = 4, vjust = 0.5)
  }
  
  # Create a summary plot showing raw module scores
  summary_data <- module_data %>%
    pivot_longer(cols = all_of(module_names),
                 names_to = "Module",
                 values_to = "Score")
  
  p_violin <- ggplot(summary_data, aes_string(x = group, y = "Score", fill = group)) +
    geom_violin(trim = FALSE, alpha = 0.8) +
    geom_boxplot(width = 0.1, outlier.size = 0.5) +
    facet_wrap(~ Module, scales = "free_y", ncol = 2) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none"
    ) +
    labs(title = "Raw Module Score Distribution",
         x = x_label, y = "Module Score")
  
  # Combine plots
  combined_plot <- p / p_violin + plot_layout(heights = c(1, 2))
  
  print(combined_plot)
  
  # Return results
  return(list(
    plot = combined_plot,
    z_scores = z_scores,
    p_values = p_adj,
    raw_means = mean_scores,
    module_names = module_names
  ))
}

#' Compare Module Scoring Methods
#'
#' This function compares the simple averaging method with AddModuleScore
#'
#' @export
CompareModuleScoringMethods <- function(
    sobj,
    gene_sets,
    group = "seurat_clusters",
    assay = "SCT"
) {
  library(ggplot2)
  library(patchwork)
  
  # Method 1: Simple averaging (original method)
  simple_results <- myhm_genesets4(sobj, group, "average", assay, gene_sets,
                                   title = "Simple Averaging Method")
  
  # Method 2: AddModuleScore-based
  module_results <- PlotModuleScoreHeatmap(sobj, gene_sets, group, assay,
                                           title = "AddModuleScore Method")
  
  # Create comparison plot
  comparison_data <- data.frame(
    Cluster = simple_results$Cluster,
    Method = "Simple"
  )
  
  for(gset in names(gene_sets)) {
    if(gset %in% colnames(simple_results)) {
      comparison_data[[gset]] <- simple_results[[gset]]
    }
  }
  
  module_z <- module_results$z_scores
  module_comparison <- data.frame(
    Cluster = module_z[[group]],
    Method = "AddModuleScore"
  )
  
  for(module in module_results$module_names) {
    if(module %in% colnames(module_z)) {
      module_comparison[[module]] <- module_z[[module]]
    }
  }
  
  # Combine data
  all_data <- bind_rows(comparison_data, module_comparison)
  
  # Create faceted comparison plot
  all_data_long <- all_data %>%
    pivot_longer(cols = -c(Cluster, Method),
                 names_to = "GeneSet",
                 values_to = "ZScore")
  
  p_compare <- ggplot(all_data_long, aes(x = Cluster, y = ZScore, fill = Method)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_wrap(~ GeneSet, scales = "free_y") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Comparison of Scoring Methods",
         x = "Cluster", y = "Z-Score")
  
  print(p_compare)
  
  return(list(
    simple = simple_results,
    module = module_results,
    comparison_plot = p_compare
  ))
}

# Example usage:
# gene_sets <- list(
#   Tcell = c("CD3D", "CD3E", "CD3G"),
#   Bcell = c("MS4A1", "CD79A", "CD79B"),
#   Monocyte = c("CD14", "LYZ", "S100A8", "S100A9")
# )
# 
# results <- PlotModuleScoreHeatmap(pbmc, gene_sets, show_pval = TRUE)
# comparison <- CompareModuleScoringMethods(pbmc, gene_sets)

#' @title Add Gene Signature Score using enrichIt (Improved Version)
#' @description Calculates a gene signature score using escape::enrichIt and adds it to the Seurat object's metadata.
#' This version flexibly accepts different gene ID types (e.g., ENSEMBL, SYMBOL, ENTREZID).
#'
#' @param seurat_obj A Seurat object.
#' @param gene_source A character string specifying the path to a file OR an R object (data.frame, vector) containing gene IDs.
#' @param signature_name A character string for the name of the new metadata column.
#' @param input_keytype The type of the input gene IDs. Must be a valid keytype for org.Hs.eg.db. 
#'                      Common values are "ENSEMBL", "SYMBOL", "ENTREZID". Defaults to "ENSEMBL".
#' @param gene_col If `gene_source` is a file path or a data.frame, specify the column index or name. Defaults to 1.
#' @param sheet_name If `gene_source` is an xlsx file, specify the sheet name or index. Defaults to 1.
#' @param assay The assay to use from the Seurat object. Defaults to "RNA".
#' @param layer The layer (slot) to use from the assay. Defaults to "data".
#' @param ... Additional arguments to be passed to `escape::enrichIt`.
#'
#' @return A Seurat object with the new signature score added to its metadata.
#'
#' @examples
#' \dontrun{
#' # Excel 파일에서 ENSEMBL ID를 읽어서 InflammatoryScore 시그니처 점수를 추가하는 예시
#' data_seurat <- add_signature_enrichit(
#'   seurat_obj    = data_seurat,
#'   gene_source   = "/data/kjc1/mylit/projects/mIBD/41590_2024_1994_MOESM6_ESM.xlsx",
#'   signature_name = "InflammatoryScore",
#'   input_keytype  = "ENSEMBL",
#'   gene_col       = 1,
#'   sheet_name     = "Inflammation_score",
#'   assay          = "RNA",
#'   layer          = "data"
#' )
#' }
#' @export
add_signature_enrichit <- function(seurat_obj,
                                      gene_source,
                                      signature_name,
                                      input_keytype = "ENSEMBL", # <--- 이 파라미터 추가!
                                      gene_col = 1,
                                      sheet_name = 1,
                                      assay = "RNA",
                                      layer = "data",
                                      ...) {
  # ... (이전 버전과 동일한 유전자 목록 로드 부분) ...
  if (is.character(gene_source) && length(gene_source) == 1 && file.exists(gene_source)) {
     # 파일 경로일 때만 이 분기
    ext <- tools::file_ext(gene_source)
    gene_list_raw <- switch(ext,
                            xlsx = read_xlsx(gene_source, sheet = sheet_name)%>% pull(gene_col),
                            csv = read.csv(gene_source, stringsAsFactors = FALSE)%>% pull(gene_col),
                            txt = read.table(gene_source, stringsAsFactors = FALSE)%>% pull(gene_col),
                            stop("Unsupported file type.")
    )
  } else if (is.data.frame(gene_source)) {
    gene_list_raw <- gene_source%>% pull(gene_col)
  } else if (is.vector(gene_source)) {
    gene_list_raw <- gene_source
  } else {
    stop("`gene_source` must be a valid file path, data.frame, or vector.")
  }
  
  # 입력된 keytype을 사용하여 항상 SYMBOL로 변환
  # 자동 감지는 권하지 않는다. 왜냐면 오류의 원인이 되기 쉽다. 데이터 정제는 따로 수행하는 것이 좋다.
  
  message(paste("Input keytype is", input_keytype, ". Converting to Gene Symbols..."))
  gene_symbols <- mapIds(
    org.Hs.eg.db,
    keys = base::unique(na.omit(as.character(gene_list_raw))),
    keytype = input_keytype, # 사용자가 지정한 ID 타입을 사용
    column = "SYMBOL",       # 최종 목표는 항상 SYMBOL
    multiVals = "first"
  )
  gene_symbols <- na.omit(gene_symbols)
  # ---
  
  if (length(gene_symbols) == 0) {
    stop(paste("No valid gene symbols could be mapped using keytype:", input_keytype))
  }
  message(paste(length(gene_symbols), "gene symbols were successfully mapped."))
  
  gene_set <- GeneSet(gene_symbols, setName = signature_name)
  gene_sets_collection <- GeneSetCollection(gene_set)
  
  message("Running enrichIt...")
  expr_matrix <- GetAssayData(seurat_obj, assay = assay, layer = layer)
  enrichment_scores <- enrichIt(
    obj = expr_matrix,
    gene.sets = gene_sets_collection,
    ...
  )
  
  message("Adding scores to Seurat metadata...")
  seurat_obj <- AddMetaData(
    object = seurat_obj,
    metadata = as.data.frame(enrichment_scores)
  )
  
  return(seurat_obj)
}

#' @title Add Pathway Activity Scores using progeny
#' @description Infers pathway activities using progeny and adds them to the Seurat object's metadata.
#'
#' @param seurat_obj A Seurat object.
#' @param organism A character string specifying the organism. Can be "Human" or "Mouse". Defaults to "Human".
#' @param topn An integer specifying the number of top genes to use for each pathway. Defaults to 100.
#' @param ... Additional arguments to be passed to `progeny::progeny`.
#'
#' @return A Seurat object with pathway activity scores added to its metadata.
#' @export
#'
#' @examples
#' \dontrun{
#' # 예시 데이터 생성
#' pbmc_small <- SeuratObject::pbmc_small
#' pbmc_with_progeny <- add_progeny_scores(pbmc_small, organism = "Human")
#'
#' # 추가된 메타데이터 확인 (14개 경로)
#' head(pbmc_with_progeny@meta.data)
#'
#' # DimPlot으로 특정 경로 활성도 시각화
#' DimPlot(pbmc_with_progeny, reduction = "umap", group.by = "seurat_clusters", label = TRUE)
#' FeaturePlot(pbmc_with_progeny, features = "NFkB")
#' }
add_progeny_scores <- function(seurat_obj, organism = "Human", topn = 100, ...) {
  
  # 1. Progeny를 실행하여 새로운 assay 추가
  message("Running progeny...")
  seurat_obj <- progeny(
    seurat_obj,
    scale = FALSE, # Scaling은 ScaleData에서 별도 수행
    organism = organism,
    topn = topn,
    return_assay = TRUE,
    ...
  )
  
  # 2. Progeny assay를 스케일링
  message("Scaling progeny assay...")
  seurat_obj <- ScaleData(seurat_obj, assay = "progeny")
  
  # 3. 스케일링된 데이터를 메타데이터로 변환하여 추가
  message("Adding scores to Seurat metadata...")
  progeny_scores <- as.data.frame(t(GetAssayData(seurat_obj, assay = "progeny", layer = "scale.data")))
  
  seurat_obj <- AddMetaData(
    object = seurat_obj,
    metadata = progeny_scores
  )
  
  return(seurat_obj)
}





# # Helper function to score new data with signature
# #' @export
# score_signature1 <- function(expr_data, signature, normalize=TRUE) {
#   genes <- signature$genes
#   weights <- signature$weights
  
#   # Extract expression matrix
#   if (inherits(expr_data, "Seurat")) {
#     expr_mat <- as.matrix(Seurat::GetAssayData(expr_data, layer="data"))
#   } else {
#     expr_mat <- as.matrix(expr_data)
#   }
  
#   # Check gene availability
#   available_genes <- base::intersect(genes, rownames(expr_mat))
#   if (length(available_genes) == 0) {
#     stop("None of the signature genes found in data")
#   }
#   if (length(available_genes) < length(genes)) {
#     warning(sprintf("%d/%d signature genes not found in data", 
#                     length(genes) - length(available_genes), length(genes)))
#   }
  
#   # Calculate scores
#   weights <- weights[available_genes]
#   scores <- colSums(expr_mat[available_genes, , drop=FALSE] * weights)
  
#   if (normalize) {
#     scores <- scale(scores)[,1]
#   }
  
#   return(scores)
# }

# Helper function to score new data with signature (ver.2)
#' @export
score_signature2 <- function(expr_data, signature, normalize=TRUE) {
  # version2: as.matrix 제거,
  genes <- signature$genes
  weights <- signature$weights
  
  # === 1. 표현 행렬 추출 (SPARSE 유지) ===
  if (inherits(expr_data, "Seurat")) {
    if (!requireNamespace("Seurat", quietly = TRUE)) {
      stop("Seurat package required.")
    }
    # as.matrix() 호출 제거!
    expr_mat <- Seurat::GetAssayData(expr_data, layer="data")
  } else {
    # 이미 행렬(희소 또는 조밀)이라고 가정
    expr_mat <- expr_data
  }
  
  # === 2. 유전자 필터링 ===
  available_genes <- base::intersect(genes, rownames(expr_mat))
  
  if (length(available_genes) == 0) {
    stop("None of the signature genes found in data")
  }
  if (length(available_genes) < length(genes)) {
    warning(sprintf("%d/%d signature genes not found in data", 
                    length(genes) - length(available_genes), length(genes)))
  }
  
  # === 3. 스코어 계산 (희소 행렬에서도 잘 작동) ===
  weights <- weights[available_genes]
  
  # drop=FALSE는 1개 유전자만 선택되어도 행렬 구조를 유지시킴
  scores <- colSums(expr_mat[available_genes, , drop=FALSE] * weights)
  
  if (normalize) {
    scores <- scale(scores)[,1]
  }
  
  return(scores)
}

#' Print Method for Gene Signature Objects
#'
#' @param x A gene_signature object
#' @param ... Additional arguments (not used)
#'
#' @export
print.gene_signature <- function(x, ...) {
  cat("Gene Signature Object\n")
  cat("====================\n")
  cat(sprintf("Method: %s\n", x$method))
  cat(sprintf("Target variable: %s (%d groups)\n", x$target_var, x$n_groups))
  cat(sprintf("Number of cells: %d\n", x$n_cells))
  cat(sprintf("Number of genes in signature: %d\n", length(x$genes)))
  cat(sprintf("\nTop 10 genes:\n"))
  print(head(data.frame(gene=x$genes, weight=x$weights[x$genes]), 10))
  
  if (!is.null(x$performance)) {
    cat("\nPerformance:\n")
    print(x$performance)
  }
}



#' Preprocess expression data for FGS v5.x
#' @keywords internal
fgs_preprocess_data_v5 <- function(data, 
                                   meta.data, 
                                   target_var, 
                                   target_group, 
                                   control_vars, 
                                   test_n, 
                                   preprocess,
                                   min_cells,
                                   min_pct,
                                   methods_requiring_scale,
                                   methods_requiring_correction) {
  
  is_seurat <- inherits(data, "Seurat")
  
  if (is_seurat) {
    if (!requireNamespace("Seurat", quietly = TRUE)) {
      stop("Seurat package required but not installed")
    }
    if (is.null(meta.data)) {
      meta.data <- data@meta.data
    }
    default_assay <- Seurat::DefaultAssay(data)
    expr_mat <- tryCatch({
      as.matrix(Seurat::GetAssayData(data, assay = default_assay, layer = "data"))
    }, error = function(e) {
      tryCatch({
        as.matrix(Seurat::GetAssayData(data, assay = default_assay, slot = "data"))
      }, error = function(e) {
        as.matrix(Seurat::GetAssayData(data, assay = default_assay, slot = "counts"))
      })
    })
  } else {
    if (is.null(meta.data)) {
      stop("meta.data must be provided when data is not a Seurat object")
    }
    expr_mat <- as.matrix(data)
  }
  
  message("V5 Preprocessing: 1. Cleaning NA values...")
  
  vars_to_check <- if (is.null(control_vars)) target_var else c(target_var, control_vars)
  complete_cases_idx <- complete.cases(meta.data[, vars_to_check, drop = FALSE])
  
  n_removed <- sum(!complete_cases_idx)
  if (n_removed > 0) {
    warning(sprintf("V5 Preprocessing: Removed %d cells with NAs in target/control vars.", n_removed))
  }
  
  meta.data <- meta.data[complete_cases_idx, , drop = FALSE]
  
  if (is.null(rownames(meta.data))) {
    stop("meta.data must have rownames corresponding to cell IDs.")
  }
  if (is.null(colnames(expr_mat))) {
    stop("Expression matrix must have column names corresponding to cell IDs.")
  }
  
  common_cells <- base::intersect(colnames(expr_mat), rownames(meta.data))
  if (length(common_cells) == 0) {
    stop("No overlapping cells between expression data and metadata.")
  }
  meta.data <- meta.data[common_cells, , drop = FALSE]
  expr_mat <- expr_mat[, common_cells, drop = FALSE]
  
  target_values <- meta.data[[target_var]]
  if (is.numeric(target_values)) {
    if (is.null(target_group)) {
      target_group <- 0.25
    }
    if (is.list(target_group)) {
      low_cutoff <- stats::quantile(target_values, target_group$low, na.rm = TRUE)
      high_cutoff <- stats::quantile(target_values, target_group$high, na.rm = TRUE)
    } else if (length(target_group) == 1 && is.numeric(target_group) && target_group < 1) {
      low_cutoff <- stats::quantile(target_values, target_group, na.rm = TRUE)
      high_cutoff <- stats::quantile(target_values, 1 - target_group, na.rm = TRUE)
    } else {
      low_cutoff <- target_group
      high_cutoff <- target_group
    }
    group_labels <- ifelse(target_values <= low_cutoff, "Low",
                           ifelse(target_values >= high_cutoff, "High", NA))
    keep_cells <- !is.na(group_labels)
    if (!all(keep_cells)) {
      warning(sprintf("Discarding %d cells outside numeric target thresholds.", sum(!keep_cells)))
      expr_mat <- expr_mat[, keep_cells, drop = FALSE]
      meta.data <- meta.data[keep_cells, , drop = FALSE]
      group_labels <- group_labels[keep_cells]
    }
    target_binary <- factor(group_labels)
  } else {
    if (!is.null(target_group)) {
      keep_cells <- target_values %in% target_group
      if (!all(keep_cells)) {
        warning(sprintf("Discarding %d cells not in target groups.", sum(!keep_cells)))
        expr_mat <- expr_mat[, keep_cells, drop = FALSE]
        meta.data <- meta.data[keep_cells, , drop = FALSE]
        target_values <- target_values[keep_cells]
      }
    }
    target_binary <- droplevels(factor(target_values))
  }
  meta.data$target_binary_var <- target_binary
  
  if (!is.null(control_vars)) {
    for (cv in control_vars) {
      if (is.factor(meta.data[[cv]])) {
        meta.data[[cv]] <- droplevels(meta.data[[cv]])
      }
    }
  }
  
  if (length(base::unique(target_binary)) < 2) {
    stop("Target variable must have at least 2 groups after processing")
  }
  
  message("V5 Preprocessing: 2. Filtering genes (min_cells, min_pct)...")
  n_cells_expr <- rowSums(expr_mat > 0)
  pct_cells_expr <- n_cells_expr / ncol(expr_mat)
  keep_genes <- (n_cells_expr >= min_cells) & (pct_cells_expr >= min_pct)
  expr_mat <- expr_mat[keep_genes, , drop = FALSE]
  
  if (nrow(expr_mat) == 0) stop("No genes pass filtering criteria")
  
  if (!is.null(test_n) && nrow(expr_mat) > test_n) {
    message(sprintf("V5 Preprocessing: 3. Pre-filtering to top %d genes (limma)...", test_n))
    if (!requireNamespace("limma", quietly = TRUE)) {
      stop("limma package required for 'test_n' pre-filtering")
    }
    
    if (is.null(control_vars)) {
      design_test <- stats::model.matrix(~ target_binary_var, data = meta.data)
    } else {
      formula_test <- stats::as.formula(paste("~ target_binary_var +", paste(control_vars, collapse = "+")))
      design_test <- stats::model.matrix(formula_test, data = meta.data)
    }
    
    fit_test <- limma::lmFit(expr_mat, design_test)
    fit_test <- limma::eBayes(fit_test)
    coef_indices <- grep("target_binary_var", colnames(design_test))
    
    top_table_test <- limma::topTable(fit_test, coef = coef_indices, number = Inf, sort.by = "P")
    
    top_gene_names <- rownames(top_table_test)[1:min(test_n, nrow(top_table_test))]
    expr_mat <- expr_mat[top_gene_names, , drop = FALSE]
    message(sprintf("... reduced to %d genes.", nrow(expr_mat)))
  }
  
  message("V5 Preprocessing: 4. Applying log1p and scaling...")
  
  if (preprocess && max(expr_mat) > 100) {
    expr_mat <- log1p(expr_mat)
  }
  
  expr_mat_scaled <- NULL
  if (length(methods_requiring_scale) > 0) {
    gene_means <- rowMeans(expr_mat)
    gene_sds <- apply(expr_mat, 1, sd)
    gene_sds[gene_sds == 0] <- 1
    expr_mat_scaled <- (expr_mat - gene_means) / gene_sds
  }
  
  message("V5 Preprocessing: 5. Applying confounder correction (if needed)...")
  
  expr_mat_corrected <- NULL
  if (!is.null(control_vars) && length(methods_requiring_correction) > 0) {
    if (!requireNamespace("limma", quietly = TRUE)) {
      stop("limma package required for removeBatchEffect")
    }
    
    covariates_df <- meta.data[, control_vars, drop = FALSE]
    covariate_mat <- stats::model.matrix(~ . - 1, data = covariates_df) 
    expr_mat_corrected <- limma::removeBatchEffect(expr_mat, covariates = covariate_mat)
  }
  
  list(
    expr_mat = expr_mat,
    expr_mat_scaled = expr_mat_scaled,
    expr_mat_corrected = expr_mat_corrected,
    meta.data = meta.data,
    target_binary = target_binary,
    n_groups = length(base::unique(target_binary)),
    control_vars = control_vars
  )
}

#' Find Gene Signature (FGS)
#'
#' @description
#' Alias for `find_gene_signature_v5.4`. A comprehensive gene signature
#' discovery function supporting multiple methods (tree-based, regularization,
#' dimensionality reduction, statistical modeling). Supports both single-cell
#' and pseudobulk data.
#'
#' @param ... All arguments passed to `find_gene_signature_v5.4`
#' @return See `find_gene_signature_v5.4`
#' @seealso [find_gene_signature_v5.4], [find_gene_signature_v5.3]
#' @export
FGS <- function(...) {
  find_gene_signature_v5.4(...)
}

#' Find Gene signature v5.4 (bug fixes for ranger/glmnet/NMF paths)
#'
#' @description Wrapper around [find_gene_signature_v5_impl] that enables the
#' `method_impl = "v5.4"` execution path, activating the latest stability
#' fixes for ranger/glmnet/NMF.
#'
#' @inheritParams find_gene_signature_v5.3
#' @return Same as [find_gene_signature_v5.3]
#' @seealso [find_gene_signature_v5.3]
#' @export
find_gene_signature_v5.4 <- function(...) {
  find_gene_signature_v5_impl(..., method_impl = "v5.4")
}

#' @export
find_gene_signature_v5.3 <- function(data, 
                                 meta.data = NULL,
                                 target_var,
                                 target_group = NULL,
                                 control_vars = NULL,   
                                 method = c("random_forest", "random_forest_ranger",
                                            "lasso", "ridge", "elastic_net",
                                            "pca_loadings", "nmf_loadings",
                                            "gam", "limma", "wilcoxon",
                                            "xgboost"),
                                 n_features = 50,
                                 test_n = NULL,         
                                 preprocess = TRUE,
                                 min_cells = 10,
                                 min_pct = 0.01,
                                 return_model = FALSE,
                                 fgs_seed = 42,
                                 lambda_selection = "lambda.1se",
                                 enet.alpha = 0.5,
                                 pca.n_pcs = 1,
                                 gam.min_unique = 15,
                                 gam.k = NULL,
                                 gam.k_dynamic_factor = 5,
                                 ...) {
  find_gene_signature_v5_impl(
    data = data,
    meta.data = meta.data,
    target_var = target_var,
    target_group = target_group,
    control_vars = control_vars,
    method = method,
    n_features = n_features,
    test_n = test_n,
    preprocess = preprocess,
    min_cells = min_cells,
    min_pct = min_pct,
    return_model = return_model,
    fgs_seed = fgs_seed,
    lambda_selection = lambda_selection,
    enet.alpha = enet.alpha,
    pca.n_pcs = pca.n_pcs,
    gam.min_unique = gam.min_unique,
    gam.k = gam.k,
    gam.k_dynamic_factor = gam.k_dynamic_factor,
    method_impl = "v5.3",
    ...
  )
}

find_gene_signature_v5_impl <- function(data, 
                                 meta.data = NULL,
                                 target_var,
                                 target_group = NULL,
                                 control_vars = NULL,   
                                 method = c("random_forest", "random_forest_ranger",
                                            "lasso", "ridge", "elastic_net",
                                            "pca_loadings", "nmf_loadings",
                                            "gam", "limma", "wilcoxon",
                                            "xgboost"),
                                 n_features = 50,
                                 test_n = NULL,         
                                 preprocess = TRUE,
                                 min_cells = 10,
                                 min_pct = 0.01,
                                 return_model = FALSE,
                                 fgs_seed = 42,
                                 # --- 신규/수정된 인자 ---
                                 lambda_selection = "lambda.1se",
                                 enet.alpha = 0.5,        # (Elastic Net용)
                                 pca.n_pcs = 1,           # (PCA용)
                                 gam.min_unique = 15,     # (GAM용)
                                 gam.k = NULL,
                                 gam.k_dynamic_factor = 5,
                                 method_impl = c("v5.3","v5.4"),
                                 ...) {
  
  method_impl <- match.arg(method_impl)
  
  # [Req 5] 메서드 순서 및 이름 변경
  all_methods <- c(
    # 1. Tree-based
    "random_forest", "random_forest_ranger", "xgboost",
    # 2. Regularization
    "lasso", "ridge", "elastic_net",
    # 3. Loadings / Dimensionality Reduction
    "pca_loadings", "nmf_loadings", # nmf -> nmf_loadings
    # 4. Statistical Modelling
    "gam", "limma", "wilcoxon"
  )
  
  if (is.null(method)) {
    method <- all_methods
  }
  
  use_dynamic_k <- is.null(gam.k)
  
  # === 1. [V5] 전처리 (단 1회 실행) ===
  
  # [Req 4] 신규 모델 스케일링 요구사항 업데이트
  methods_requiring_scale <- c("lasso", "ridge", "elastic_net", 
                               "gam", "pca_loadings", "xgboost")
  methods_requiring_correction <- c("wilcoxon", "pca_loadings")
  
  preprocessed_data <- fgs_preprocess_data_v5(
    data = data, meta.data = meta.data, target_var = target_var, 
    target_group = target_group, control_vars = control_vars, 
    test_n = test_n, preprocess = preprocess, min_cells = min_cells,
    min_pct = min_pct,
    methods_requiring_scale = base::intersect(method, methods_requiring_scale),
    methods_requiring_correction = base::intersect(method, methods_requiring_correction)
  )

  expr_mat_base <- preprocessed_data$expr_mat
  meta.data_clean <- preprocessed_data$meta.data
  target_binary <- preprocessed_data$target_binary
  n_groups <- preprocessed_data$n_groups
  
  set.seed(fgs_seed)
  
  covariate_mat_model <- NULL
  if (!is.null(control_vars)) {
     covariates_df_model <- meta.data_clean[, control_vars, drop = FALSE]
     covariate_mat_model <- model.matrix(~ . - 1, data = covariates_df_model)
  }

  results_list <- list()
  
  run_glmnet_signature <- function(X, alpha_value, apply_v54 = FALSE, ...) {
    if (!requireNamespace("glmnet", quietly = TRUE)) {
      stop("glmnet package required. Install with: install.packages('glmnet')")
    }
    
    if (is.null(control_vars)) {
      X_model <- X
      penalty_vec <- rep(1, ncol(X_model))
    } else {
      X_model <- cbind(X, covariate_mat_model)
      penalty_vec <- c(rep(1, ncol(X)), rep(0, ncol(covariate_mat_model)))
    }
    
    X_model <- as.matrix(X_model)
    family <- if (n_groups == 2) "binomial" else "multinomial"
    y_glmnet <- if (n_groups == 2 && apply_v54) as.numeric(y) - 1 else y
    
    cv_fit <- glmnet::cv.glmnet(
      X_model, y_glmnet,
      family = family,
      alpha = alpha_value,
      penalty.factor = penalty_vec,
      ...
    )
    
    if (n_groups == 2) {
      coef_obj <- coef(cv_fit, s = lambda_selection)
      idx <- seq_len(ncol(X)) + 1
      weights_all <- as.numeric(coef_obj[idx])
      names(weights_all) <- rownames(coef_obj)[idx]
    } else {
      coef_list <- coef(cv_fit, s = lambda_selection)
      idx <- seq_len(ncol(X)) + 1
      weights_mat <- vapply(coef_list, function(mat) as.numeric(mat[idx]), numeric(length(idx)))
      weights_all <- rowMeans(weights_mat)
      names(weights_all) <- rownames(coef_list[[1]])[idx]
    }
    
    weights_all <- weights_all[!is.na(weights_all)]
    if (apply_v54) {
      weights_all <- weights_all[is.finite(weights_all)]
    }
    
    if (length(weights_all) == 0) {
      stop("glmnet returned no gene coefficients.")
    }
    
    top_genes <- names(sort(abs(weights_all), decreasing = TRUE)[1:min(n_features, length(weights_all))])
    weights <- weights_all[top_genes]
    
    scores <- as.numeric(X[, top_genes, drop = FALSE] %*% weights)
    names(scores) <- rownames(X)
    
    pred_probs <- predict(cv_fit, newx = X_model, s = lambda_selection, type = "response")
    if (n_groups == 2) {
      pred <- factor(ifelse(pred_probs > 0.5, levels(y)[2], levels(y)[1]), levels = levels(y))
      if (requireNamespace("pROC", quietly = TRUE)) {
        roc_obj <- pROC::roc(y, scores, quiet = TRUE) 
        auc <- as.numeric(pROC::auc(roc_obj))
      } else { auc <- NA }
      acc <- mean(pred == y)
      perf <- list(accuracy = acc, auc = auc, confusion = table(pred, y))
    } else {
      pred_class_indices <- apply(pred_probs[,,1], 1, which.max)
      pred <- levels(y)[pred_class_indices]
      acc <- mean(pred == y)
      perf <- list(accuracy = acc, confusion = table(pred, y))
    }
    
    list(
      genes = top_genes,
      weights = weights,
      scores = scores,
      performance = perf,
      model = if (return_model) cv_fit else NULL
    )
  }
  
  
  # === 2. [V5] 메서드 순회 (for 루프) ===
  
  for (m in method) {
    
    if (!m %in% all_methods) {
      warning(sprintf("Invalid method '%s'. Skipping.", m))
      next
    }
    
    message(sprintf("--- Running Method: %s ---", m))
    
    tryCatch({
      
      # === 3. 데이터 선택 (메서드별) ===
      
      if (m %in% c("limma", "wilcoxon", "nmf_loadings", "random_forest", "random_forest_ranger")) {
        expr_mat_method <- expr_mat_base
      } else if (m %in% c("lasso", "ridge", "elastic_net", "gam", "pca_loadings", "xgboost")) {
        # 'expr_mat_scaled'가 NULL이면 (필요한 메서드가 없었으면) 원본 사용
        expr_mat_method <- if(is.null(preprocessed_data$expr_mat_scaled)) expr_mat_base else preprocessed_data$expr_mat_scaled
      }
      
      if (m %in% c("wilcoxon", "pca_loadings")) {
        if (!is.null(preprocessed_data$expr_mat_corrected)) {
          expr_mat_method <- preprocessed_data$expr_mat_corrected
        }
      }
      
      # (pca_loadings는 스케일링된 보정 데이터가 이상적이나, v5.2는 보정된 비-스케일링 데이터를 우선함)

      X <- t(expr_mat_method)
      y <- target_binary

      
      # === 4. Method-specific (v5.2 수정) ===
      
      result <- switch(m,
        
        # --- 1. Tree-based ---
        
        random_forest = { # [Req 3] 이름 변경
          if (!requireNamespace("randomForest", quietly = TRUE)) {
            stop("randomForest package required.")
          }
          
          # RF는 스케일링되지 않은 원본 X 사용
          X_rf <- t(expr_mat_base) 
          if (!is.null(control_vars)) { X_rf <- cbind(X_rf, covariate_mat_model) }
          
          rf_model <- randomForest::randomForest(x = X_rf, y = y, ntree = 500, importance = TRUE, ...)
          
          importance_scores <- randomForest::importance(rf_model)
          if (n_groups == 2) {
            weights_magnitude_all <- importance_scores[, "MeanDecreaseGini"]
          } else {
            weights_magnitude_all <- rowMeans(importance_scores[, grep("MeanDecreaseGini", colnames(importance_scores))])
          }
          
          gene_names_in_model <- colnames(X_rf)[!colnames(X_rf) %in% colnames(covariate_mat_model)]
          weights_magnitude_genes <- weights_magnitude_all[gene_names_in_model]
          
          top_genes <- names(sort(weights_magnitude_genes, decreasing=TRUE)[1:min(n_features, length(weights_magnitude_genes))])
          weights_magnitude <- weights_magnitude_genes[top_genes]
          
          # 방향성 보정 및 performance 계산
          if (n_groups == 2) {
            g1_cells <- y == levels(y)[1]
            g2_cells <- y == levels(y)[2]
            mean_g1 <- colMeans(X_rf[g1_cells, top_genes, drop=FALSE])
            mean_g2 <- colMeans(X_rf[g2_cells, top_genes, drop=FALSE])
            effect_size <- mean_g2 - mean_g1 
            weights <- weights_magnitude * sign(effect_size)
          } else {
            warning("random_forest: n_groups > 2. Score represents magnitude (importance), not direction.")
            weights <- weights_magnitude
          }
          
          scores <- as.numeric(X_rf[, top_genes] %*% weights)
          names(scores) <- rownames(X_rf)
          
          pred <- rf_model$predicted
          if (n_groups == 2) {
            if (requireNamespace("pROC", quietly = TRUE)) {
              roc_obj <- pROC::roc(y, scores, quiet=TRUE)
              auc <- as.numeric(pROC::auc(roc_obj))
            } else { auc <- NA }
            acc <- mean(pred == y)
            perf <- list(accuracy = acc, auc = auc, confusion = table(pred, y))
          } else {
            acc <- mean(pred == y)
            perf <- list(accuracy = acc, confusion = table(pred, y))
          }
          
          list(genes = top_genes, weights = weights, scores = scores,
               performance = perf, model = if(return_model) rf_model else NULL)
        },
        
        random_forest_ranger = {
          if (!requireNamespace("ranger", quietly = TRUE)) {
            stop("ranger package required.")
          }
          
          X_ranger <- t(expr_mat_base) 
          if (!is.null(control_vars)) { X_ranger <- cbind(X_ranger, covariate_mat_model) }
          
          ranger_data <- data.frame(y = y, X_ranger)
          importance_mode <- if (method_impl == "v5.4") "permutation" else "impurity"
          
          rf_model <- tryCatch(
            ranger::ranger(
              y ~ ., 
              data = ranger_data, 
              num.trees = 500, 
              importance = importance_mode,
              ...
            ),
            error = function(e) {
              if (method_impl == "v5.4" && importance_mode == "permutation") {
                warning("Permutation importance failed in ranger; falling back to impurity. ", e$message)
                ranger::ranger(
                  y ~ ., 
                  data = ranger_data, 
                  num.trees = 500, 
                  importance = "impurity",
                  ...
                )
              } else {
                stop(e)
              }
            }
          )
          
          weights_magnitude_all <- ranger::importance(rf_model)
          if (!is.null(control_vars)) {
            weights_magnitude_all <- weights_magnitude_all[!names(weights_magnitude_all) %in% colnames(covariate_mat_model)]
          }
          weights_magnitude_all <- weights_magnitude_all[is.finite(weights_magnitude_all)]
          
          top_genes <- names(sort(weights_magnitude_all, decreasing=TRUE)[1:min(n_features, length(weights_magnitude_all))])
          weights_magnitude <- weights_magnitude_all[top_genes]

          if (n_groups == 2) {
            g1_cells <- y == levels(y)[1]
            g2_cells <- y == levels(y)[2]
            mean_g1 <- colMeans(X_ranger[g1_cells, top_genes, drop=FALSE])
            mean_g2 <- colMeans(X_ranger[g2_cells, top_genes, drop=FALSE])
            effect_size <- mean_g2 - mean_g1 
            weights <- weights_magnitude * sign(effect_size)
          } else {
            warning("random_forest_ranger: n_groups > 2. Score represents magnitude (importance), not direction.")
            weights <- weights_magnitude
          }
          
          scores <- as.numeric(X_ranger[, top_genes, drop=FALSE] %*% weights)
          names(scores) <- rownames(X_ranger)
          
          pred <- rf_model$predictions
          if (n_groups == 2) {
            if (requireNamespace("pROC", quietly = TRUE)) {
              roc_obj <- pROC::roc(y, scores, quiet=TRUE)
              auc <- as.numeric(pROC::auc(roc_obj))
            } else { auc <- NA }
            acc <- mean(pred == y)
            perf <- list(accuracy = acc, auc = auc, confusion = table(pred, y))
          } else {
            acc <- mean(pred == y)
            perf <- list(accuracy = acc, confusion = table(pred, y))
          }
          
          list(genes = top_genes, weights = weights, scores = scores,
               performance = perf, model = if(return_model) rf_model else NULL)
        },

        xgboost = { # [Req 4] 신규 (Beta)
          if (!requireNamespace("xgboost", quietly = TRUE)) {
            stop("xgboost package required.")
          }
          
          # X는 스케일링된 데이터 (expr_mat_scaled)
          X_xgb <- X 
          if (!is.null(control_vars)) { X_xgb <- cbind(X_xgb, covariate_mat_model) }
          
          # y를 0/1 숫자로 변환
          y_numeric <- as.numeric(y) - 1
          
          dtrain <- xgboost::xgb.DMatrix(data = X_xgb, label = y_numeric)
          
          params <- list(
            objective = if (n_groups == 2) "binary:logistic" else "multi:softmax",
            eval_metric = if (n_groups == 2) "logloss" else "mlogloss",
            nthread = 4,
            eta = 0.1
          )
          if (n_groups > 2) { params$num_class = n_groups }
          
          xgb_model <- xgboost::xgb.train(params, dtrain, nrounds = 100, ...)
          
          imp_matrix <- xgboost::xgb.importance(model = xgb_model)
          weights_magnitude_all <- imp_matrix$Gain
          names(weights_magnitude_all) <- imp_matrix$Feature
          
          gene_names_in_model <- names(weights_magnitude_all)[!names(weights_magnitude_all) %in% colnames(covariate_mat_model)]
          weights_magnitude_genes <- weights_magnitude_all[gene_names_in_model]
          
          top_genes <- names(sort(weights_magnitude_genes, decreasing=TRUE)[1:min(n_features, length(weights_magnitude_genes))])
          weights_magnitude <- weights_magnitude_genes[top_genes]

          # 방향성 보정 및 performance 계산
          if (n_groups == 2) {
            g1_cells <- y == levels(y)[1]
            g2_cells <- y == levels(y)[2]
            mean_g1 <- colMeans(X_xgb[g1_cells, top_genes, drop=FALSE])
            mean_g2 <- colMeans(X_xgb[g2_cells, top_genes, drop=FALSE])
            effect_size <- mean_g2 - mean_g1 
            weights <- weights_magnitude * sign(effect_size)
          } else {
            warning("xgboost: n_groups > 2. Score represents magnitude (importance), not direction.")
            weights <- weights_magnitude
          }
          
          scores <- as.numeric(X_xgb[, top_genes] %*% weights)
          names(scores) <- rownames(X_xgb)
          
          pred_probs <- predict(xgb_model, newdata = X_xgb)
          if (n_groups == 2) {
            pred <- factor(ifelse(pred_probs > 0.5, levels(y)[2], levels(y)[1]), levels=levels(y))
            if (requireNamespace("pROC", quietly = TRUE)) {
              roc_obj <- pROC::roc(y, scores, quiet=TRUE)
              auc <- as.numeric(pROC::auc(roc_obj))
            } else { auc <- NA }
            acc <- mean(pred == y)
            perf <- list(accuracy = acc, auc = auc, confusion = table(pred, y))
          } else {
            pred_class_indices <- apply(pred_probs, 1, which.max)
            pred <- levels(y)[pred_class_indices]
            acc <- mean(pred == y)
            perf <- list(accuracy = acc, confusion = table(pred, y))
          }
          
          list(genes = top_genes, weights = weights, scores = scores,
               performance = perf, model = if(return_model) xgb_model else NULL)
        },

        # --- 2. Regularization ---

        lasso = run_glmnet_signature(X, alpha_value = 1, apply_v54 = FALSE, ...),
        
        ridge = run_glmnet_signature(
          X,
          alpha_value = 0,
          apply_v54 = (method_impl == "v5.4"),
          ...
        ),
        
        elastic_net = run_glmnet_signature(
          X,
          alpha_value = enet.alpha,
          apply_v54 = (method_impl == "v5.4"),
          ...
        ),

        # --- 3. Loadings ---

        pca_loadings = {
          # [Req 2] n_pcs 로직 적용
          
          # expr_mat_method는 보정O, 스케일링X (또는 스케일링O, 보정X)
          # v5.2는 prcomp 전에 다시 스케일링을 보장
          pca_res <- prcomp(X, center=TRUE, scale.=TRUE) 
          
          n_pcs_to_test <- min(50, ncol(pca_res$x))
          pc_cors <- numeric(n_pcs_to_test)
          
          for (k in 1:n_pcs_to_test) {
            if (n_groups == 2) {
              pc_cors[k] <- abs(cor(pca_res$x[, k], as.numeric(target_binary)))
            } else {
              pc_cors[k] <- summary(aov(pca_res$x[, k] ~ target_binary))[[1]][1, "F value"]
            }
          }
          
          # 1. 가장 상관관계 높은 (Best) PC 1개
          best_pc_idx <- which.max(pc_cors)
          
          # 2. 상위 N개 (Top N) PC
          top_n_pcs_indices <- order(pc_cors, decreasing=TRUE)[1:min(pca.n_pcs, n_pcs_to_test)]
          
          # 3. 중요도 계산: Top N개 PC의 로딩 절대값 합
          top_pc_loadings <- pca_res$rotation[, top_n_pcs_indices, drop=FALSE]
          weights_magnitude_all <- rowSums(abs(top_pc_loadings))
          
          # 4. 방향성 계산: Best PC 1개의 부호있는 로딩
          weights_directional_all <- pca_res$rotation[, best_pc_idx]
          
          # 5. 유전자 선별: Top N개 기준
          top_genes <- names(sort(weights_magnitude_all, decreasing=TRUE)[1:min(n_features, length(weights_magnitude_all))])
          
          # 6. 최종 가중치: Best PC 1개의 방향성 적용
          weights <- weights_directional_all[top_genes]
          
          scores <- as.numeric(X[, top_genes] %*% weights)
          names(scores) <- rownames(X)
          
          perf <- list(Best_PC = best_pc_idx, 
                       Top_N_PCs = top_n_pcs_indices,
                       Top_N_Correlations = pc_cors[top_n_pcs_indices],
                       Best_PC_VarExplained = summary(pca_res)$importance[2, best_pc_idx])
          
          list(genes = top_genes, weights = weights, scores = scores,
               performance = perf, model = if(return_model) pca_res else NULL)
        },

        nmf_loadings = {
          if (!requireNamespace("NMF", quietly = TRUE)) {
            stop("NMF package required.")
          }
          
          expr_mat_nmf <- expr_mat_method 
          
          if (!is.null(control_vars)) {
            warning(sprintf("Method NMF: Applying limma::removeBatchEffect for: %s...", m))
            covariate_mat <- model.matrix(~ . - 1, data = meta.data_clean[, control_vars, drop=FALSE])
            expr_mat_nmf <- limma::removeBatchEffect(expr_mat_nmf, covariates = covariate_mat)
          }

          min_expr <- suppressWarnings(min(expr_mat_nmf, na.rm = TRUE))
          eps <- 1e-3
          shift <- if (method_impl == "v5.4") {
            if (is.finite(min_expr) && min_expr <= 0) abs(min_expr) + eps else eps
          } else {
            -min_expr + 0.01
          }
          expr_mat_pos <- expr_mat_nmf + shift
          
          rank_cap <- min(n_groups + 2, 10)
          if (method_impl == "v5.4") {
            max_rank_allowed <- min(nrow(expr_mat_pos) - 1, ncol(expr_mat_pos) - 1)
            if (!is.finite(max_rank_allowed) || max_rank_allowed < 2) {
              stop("nmf_loadings: insufficient dimensions to fit requested rank.")
            }
            rank <- max(2, min(rank_cap, max_rank_allowed))
          } else {
            rank <- rank_cap
          }
          dots <- list(...)
          
          # [Req 6 FIX] '...' (dots)에서 NMF 유효 인자만 필터링
          valid_nmf_args <- c("nrun", ".options", ".pbackend") 
          filtered_dots <- dots[names(dots) %in% valid_nmf_args]
          
          nmf_args <- c(list(x = expr_mat_pos, 
                             rank = rank, 
                             seed = fgs_seed, 
                             method = "brunet"), # 기본 알고리즘 명시
                        filtered_dots)
          
          nmf_res <- do.call(NMF::nmf, nmf_args)
          
          W <- NMF::basis(nmf_res)
          H <- NMF::coef(nmf_res)
          
          component_cors <- numeric(rank)
          for (k in 1:rank) {
            if (n_groups == 2) {
              component_cors[k] <- abs(cor(H[k, ], as.numeric(target_binary)))
            } else {
              component_cors[k] <- summary(aov(H[k, ] ~ target_binary))[[1]][1, "F value"]
            }
          }
          
          best_component <- which.max(component_cors)
          weights_magnitude <- W[, best_component]
          names(weights_magnitude) <- rownames(expr_mat_pos)
          
          top_genes <- names(sort(weights_magnitude, decreasing=TRUE)[1:min(n_features, length(weights_magnitude))])
          weights_magnitude <- weights_magnitude[top_genes]
          
          if (n_groups == 2) {
            g1_cells <- y == levels(y)[1]
            g2_cells <- y == levels(y)[2]
            mean_g1 <- colMeans(X[g1_cells, top_genes, drop=FALSE])
            mean_g2 <- colMeans(X[g2_cells, top_genes, drop=FALSE])
            effect_size <- mean_g2 - mean_g1
            weights <- weights_magnitude * sign(effect_size)
          } else {
            warning("nmf_loadings: n_groups > 2. Score represents magnitude (importance), not direction.")
            weights <- weights_magnitude
          }
          
          scores <- as.numeric(X[, top_genes] %*% weights)
          names(scores) <- rownames(X)
          
          perf <- list(component = best_component, correlation = component_cors[best_component])
          
          list(genes = top_genes, weights = weights, scores = scores,
               performance = perf, model = if(return_model) nmf_res else NULL)
        },

        # --- 4. Statistical Modelling ---
        
        gam = {
          if (!requireNamespace("mgcv", quietly = TRUE)) {
            stop("mgcv package required. Install with: install.packages('mgcv')")
          }
          
          deviance_explained <- numeric(nrow(expr_mat_method))
          names(deviance_explained) <- rownames(expr_mat_method)
          
          gam_data <- data.frame(y_var_numeric = as.numeric(target_binary) - 1)
          if (!is.null(control_vars)) {
            gam_data <- cbind(gam_data, covariate_mat_model)
            formula_base <- paste(" +", paste(colnames(covariate_mat_model), collapse=" + "))
          } else {
            formula_base <- ""
          }

          convergence_warnings <- 0
          genes_skipped <- 0
          genes_with_dynamic_k <- 0
          
          for (i in 1:nrow(expr_mat_method)) {
            gam_data$gene_expr <- expr_mat_method[i, ]
            
            n_unique_vals <- length(base::unique(gam_data$gene_expr))
            if (n_unique_vals < gam.min_unique) {
              deviance_explained[i] <- 0
              genes_skipped <- genes_skipped + 1
              next
            }
            
            k_value <- if (use_dynamic_k) {
              k_dyn <- floor(n_unique_vals / gam.k_dynamic_factor)
              k_dyn <- max(3, min(10, k_dyn))
              if (k_dyn != 10) genes_with_dynamic_k <- genes_with_dynamic_k + 1
              k_dyn
            } else {
              gam.k
            }
            
            formula_str <- paste("y_var_numeric ~ s(gene_expr, k=", k_value, ", bs='cr')", formula_base)

            fit_result <- tryCatch({
              if (n_groups == 2) {
                mgcv::bam(as.formula(formula_str), data = gam_data, family="binomial", ...)
              } else {
                mgcv::bam(as.formula(formula_str), data = gam_data, ...)
              }
            }, warning = function(w) {
              if (grepl("did not converge", w$message)) {
                convergence_warnings <<- convergence_warnings + 1
              }
              invokeRestart("muffleWarning")
            }, error = function(e) {
              warning(sprintf("GAM failed for gene %s: %s", rownames(expr_mat_method)[i], e$message))
              return(NULL) 
            })
            
            if (is.null(fit_result)) {
              deviance_explained[i] <- 0
            } else {
              deviance_explained[i] <- summary(fit_result)$dev.expl
            }
          }
          
          if (genes_skipped > 0) {
            warning(sprintf("GAM: Skipped %d genes (unique values < gam.min_unique=%d).", genes_skipped, gam.min_unique))
          }
          if (convergence_warnings > 0) {
            warning(sprintf("GAM: %d genes failed to converge.", convergence_warnings))
          }
          if (use_dynamic_k && genes_with_dynamic_k > 0) {
            message(sprintf("GAM: Applied dynamic k for %d genes.", genes_with_dynamic_k))
          }
          
          weights_magnitude <- deviance_explained
          top_genes <- names(sort(weights_magnitude, decreasing=TRUE)[1:min(n_features, sum(weights_magnitude > 0, na.rm=TRUE))])
          
          if (length(top_genes) == 0) {
            warning("GAM method found no genes with deviance > 0.")
            return(list(genes=character(0), weights=numeric(0), scores=numeric(0), performance=list()))
          }
          
          weights_magnitude <- weights_magnitude[top_genes]

          if (n_groups == 2) {
            g1_cells <- y == levels(y)[1]
            g2_cells <- y == levels(y)[2]
            mean_g1 <- colMeans(X[g1_cells, top_genes, drop=FALSE])
            mean_g2 <- colMeans(X[g2_cells, top_genes, drop=FALSE])
            effect_size <- mean_g2 - mean_g1
            weights <- weights_magnitude * sign(effect_size)
          } else {
            warning("gam: n_groups > 2. Score represents magnitude (importance), not direction.")
            weights <- weights_magnitude
          }
          
          scores <- as.numeric(X[, top_genes, drop=FALSE] %*% weights)
          names(scores) <- rownames(X)
          
          perf <- list(deviance_explained = weights_magnitude)
          
          list(genes = top_genes, weights = weights, scores = scores,
               performance = perf, model = NULL)
        },
        
        limma = {
          if (!requireNamespace("limma", quietly = TRUE)) {
            stop("limma package required. Install with: BiocManager::install('limma')")
          }
          
          # Sanitize target_binary levels for limma (must be valid R names)
          target_binary_limma <- target_binary
          orig_levels <- levels(target_binary_limma)
          safe_levels <- make.names(orig_levels)
          if (!all(orig_levels == safe_levels)) {
            levels(target_binary_limma) <- safe_levels
            meta.data_clean$target_binary_limma <- target_binary_limma
          } else {
            meta.data_clean$target_binary_limma <- target_binary_limma
          }
          
          # expr_mat_method는 'expr_mat_base' (보정되지 않은 원본)
          if (is.null(control_vars)) {
            design <- model.matrix(~0 + target_binary_limma, data = meta.data_clean)
            colnames(design)[1:n_groups] <- levels(target_binary_limma)
          } else {
            control_formula <- paste(control_vars, collapse = " + ")
            full_formula <- as.formula(paste("~0 + target_binary_limma +", control_formula))
            design <- model.matrix(full_formula, data = meta.data_clean)
            colnames(design)[1:n_groups] <- levels(target_binary_limma) 
          }

          fit <- limma::lmFit(expr_mat_method, design)
          
          if (n_groups == 2) {
            contrast_str <- paste(levels(target_binary_limma)[2], levels(target_binary_limma)[1], sep="-")
          } else {
            contrast_pairs <- combn(levels(target_binary_limma), 2, function(x) paste(x[2], x[1], sep="-"))
            contrast_str <- contrast_pairs
            warning("limma: n_groups > 2. Using all pairwise contrasts. Weights based on average t-statistic.")
          }
          
          contrast_mat <- limma::makeContrasts(contrasts=contrast_str, levels=design)
          fit2 <- limma::contrasts.fit(fit, contrast_mat)
          fit2 <- limma::eBayes(fit2)
          
          if(n_groups == 2) {
              top_table <- limma::topTable(fit2, number=Inf, sort.by="B")
              weights_all <- top_table$t
          } else {
              top_table <- limma::topTable(fit2, number=Inf, sort.by="F")
              weights_all <- rowMeans(abs(fit2$t[, contrast_str, drop=FALSE]))
          }
          
          names(weights_all) <- rownames(top_table)
          
          top_genes <- rownames(top_table)[1:min(n_features, nrow(top_table))]
          
          if(n_groups == 2) {
              weights <- top_table[top_genes, "t"]
          } else {
              weights <- weights_all[top_genes]
          }

          scores <- as.numeric(X[, top_genes] %*% weights)
          names(scores) <- rownames(X)
          
          perf <- list(top_table = top_table[top_genes, ])
          
          list(genes = top_genes, weights = weights, scores = scores,
               performance = perf, model = if(return_model) fit2 else NULL)
        },
        
        wilcoxon = {
          # expr_mat_method는 'expr_mat_corrected' (보정된 버전)
          pvals <- numeric(nrow(expr_mat_method))
          effect_sizes <- numeric(nrow(expr_mat_method))
          names(pvals) <- rownames(expr_mat_method)
          names(effect_sizes) <- rownames(expr_mat_method)

          for (i in 1:nrow(expr_mat_method)) {
            if (n_groups == 2) {
              group1 <- expr_mat_method[i, target_binary == levels(target_binary)[1]]
              group2 <- expr_mat_method[i, target_binary == levels(target_binary)[2]]
              
              test <- try(wilcox.test(group1, group2), silent=TRUE) 
              
              if(inherits(test, "try-error")) {
                pvals[i] <- 1.0
                effect_sizes[i] <- 0
              } else {
                pvals[i] <- test$p.value
                effect_sizes[i] <- median(group2) - median(group1)
              }
            } else {
              test <- try(kruskal.test(expr_mat_method[i, ] ~ target_binary), silent=TRUE) 
              
              if(inherits(test, "try-error")) {
                 pvals[i] <- 1.0
                 effect_sizes[i] <- 0
              } else {
                 pvals[i] <- test$p.value
                 effect_sizes[i] <- var(tapply(expr_mat_method[i, ], target_binary, median))
              }
            }
          }
          
          padj <- p.adjust(pvals, method="BH")
          
          ranks <- rank(pvals) + rank(-abs(effect_sizes))
          top_genes <- names(sort(ranks)[1:min(n_features, length(ranks))])
          
          weights <- effect_sizes[top_genes]
          
          scores <- as.numeric(X[, top_genes] %*% weights)
          names(scores) <- rownames(X)
          
          perf <- list(pvalues = pvals[top_genes], padj = padj[top_genes],
                       effect_sizes = effect_sizes[top_genes])
          
          list(genes = top_genes, weights = weights, scores = scores,
               performance = perf, model = NULL)
        }
        
      ) # --- switch 끝 ---

      # === 5. 결과 저장 ===
      result$method <- m
      result$target_var <- target_var
      result$n_groups <- n_groups
      result$n_cells <- ncol(expr_mat_base)
      
      class(result) <- c("gene_signature", "list")
      results_list[[m]] <- result
      
    }, error = function(e) {
      warning(sprintf("Method '%s' failed with error: %s", m, e$message))
      results_list[[m]] <- list(method = m, error = e$message)
    })
    
  } # --- for 루프 끝 ---

  return(results_list)
}

#' Train Meta-Learner (TML7): Stacked Ensemble Model for Signature Scores
#'
#' @description
#'   Implements a two-level stacking approach for combining multiple Level-1 (L1)
#'   gene signatures into a unified prediction model. The function standardizes
#'   diverse signature formats (character vectors, named numeric weights,
#'   up/down lists, data frames) into a common representation, computes signature
#'   scores on the holdout expression data, and trains one or more Level-2 (L2)
#'   caret models on the resulting score matrix. The best-performing model,
#'   selected via cross-validation, is returned along with the processed training
#'   data and standardized signatures.
#'
#'   **Mechanism:**
#'   1. **Signature Standardization**: Each L1 signature is converted to a named
#'      numeric vector of gene weights using `as_signature()`, which handles
#'      multiple input formats (character vectors → uniform weights, named
#'      numeric → direct weights, up/down lists → +1/-1 weights, data frames →
#'      extracted gene-weight pairs).
#'   2. **Score Computation**: For each signature, a per-cell score is computed
#'      as the weighted average of expression values, normalized by the sum of
#'      absolute weights and optionally z-scored across cells.
#'   3. **L2 Feature Matrix**: The signature scores form the columns of the L2
#'      training matrix (one row per cell, one column per signature).
#'   4. **Model Training**: Multiple caret models (e.g., glm, ranger, xgbTree)
#'      are trained via k-fold cross-validation, with the best model selected
#'      based on the specified metric (AUC/ROC for binary, Accuracy/Kappa for
#'      multi-class).
#'   5. **Parallel Safety**: Parallel execution is opt-in (`allow_parallel=TRUE`)
#'      and guarded to prevent runaway worker creation on shared servers.
#'      Worker count is capped at 8 by default.
#'
#' @param l1_signatures Named list of L1 signatures. Each element can be:
#'   \itemize{
#'     \item A character vector of gene names (uniform weights = 1)
#'     \item A named numeric vector (gene names → weights)
#'     \item A list with `up` and/or `down` components (up-regulated genes get
#'           weight +1, down-regulated get -1)
#'     \item A data frame with gene identifiers and weight/score columns
#'   }
#' @param holdout_data Expression container. Either:
#'   \itemize{
#'     \item A Seurat object (expression extracted via `GetAssayData(layer=layer)`)
#'     \item A matrix or data frame with rows = genes/features, columns = cells/samples
#'   }
#' @param target_var For Seurat input: column name in `meta.data` containing the
#'   outcome to predict. For matrix input: the target vector itself (must match
#'   cell order).
#' @param l2_methods Character vector of caret model identifiers to evaluate
#'   (default: `c("glm", "ranger", "xgbTree")`). Supported values:
#'   `c("glm","ranger","xgbTree","glmnet","svmRadial","mlp",
#'     "mlpKerasDropout","nnet","earth")`. Any unsupported method names are
#'   dropped with a warning; each candidate requires its backing package
#'   (e.g. `glmnet`, `kernlab`, `RSNNS`, `keras`, `earth`).
#' @param k_folds Number of cross-validation folds (default: 5).
#' @param metric Performance metric for model selection. Options:
#'   \itemize{
#'     \item `"AUC"` or `"ROC"`: For binary classification (requires `twoClassSummary`)
#'     \item `"Accuracy"`: Classification accuracy
#'     \item `"Kappa"`: Cohen's kappa
#'   }
#'   For multi-class targets, AUC/ROC is invalid and falls back to Accuracy.
#' @param fgs_seed Random seed for reproducibility (default: 42).
#' @param layer When `holdout_data` is a Seurat object, the assay layer to extract
#'   (default: `"data"`). Options: `"counts"`, `"data"`, `"scale.data"`.
#' @param allow_parallel Logical. If `TRUE` and `future`/`doFuture` packages are
#'   available, enables parallel cross-validation via `future::multisession` plan
#'   (default: `FALSE` for safety).
#' @param parallel_workers Optional integer overriding the number of parallel
#'   workers when `allow_parallel=TRUE`. If `NULL`, defaults to
#'   `min(8, detectCores(logical=FALSE))`, falling back to 4 if detection fails.
#' @param cv_group_var Optional metadata column (Seurat input only) used to
#'   define group-wise cross-validation splits (default: `"emrid"`). When the
#'   column is missing, contains `NA`, or when `holdout_data` is not Seurat,
#'   the function automatically falls back to standard cell-wise CV with a log
#'   message describing the reason.
#'
#' @return A list with components:
#'   \describe{
#'     \item{best_model}{Caret `train` object for the selected L2 model.}
#'     \item{best_model_name}{Character string identifier of the best model.}
#'     \item{best_metric_name}{Name of the metric used for selection.}
#'     \item{model_comparison}{A `resamples` object comparing all trained models
#'           (or `NULL` if only one model succeeded).}
#'     \item{trained_models}{Named list of all successful caret `train` objects
#'           prior to best-model selection.}
#'     \item{l2_train}{Data frame with signature scores (columns) and target
#'           variable (`.target` column).}
#'     \item{l1_signatures}{Standardized signatures (named numeric weight vectors).}
#'     \item{positive_class}{Name of the positive class (tail of `.target`
#'           levels) stored for downstream importance utilities.}
#'   }
#'
#' @details
#'   **Signature Scoring Formula:**
#'   For a signature with gene weights \eqn{w_g} and expression values \eqn{x_{gc}}
#'   for gene \eqn{g} and cell \eqn{c}:
#'   \deqn{s_c = \frac{\sum_g w_g \cdot x_{gc}}{\sum_g |w_g|}}
#'   If `normalize=TRUE`, scores are further z-scored: \eqn{s_c' = (s_c - \mu_s) / \sigma_s}.
#'
#'   **Model Selection:**
#'   The best model is chosen by maximizing the cross-validated metric across all
#'   folds. For binary classification with AUC/ROC, `caret::twoClassSummary` is
#'   used; for multi-class or Accuracy/Kappa, `caret::defaultSummary` is used.
#'
#'   **Group-wise CV Logging:**
#'   When `cv_group_var` is available, folds are created at the group level and
#'   per-fold index/indexOut sizes (cells + groups) are logged to aid debugging.
#'   Missing metadata (or insufficient group counts) triggers an automatic
#'   fallback to standard cell-wise folds.
#'
#'   **L2 Registry:**
#'   Only registered caret methods are allowed. Dependencies are verified via
#'   `requireNamespace()` and missing packages cause the corresponding methods
#'   to be dropped before training.
#'
#' @examples
#' \dontrun{
#' # Example: Combine multiple signature-finding methods
#' sigs <- list(
#'   lasso = find_gene_signature(data, target_var="g3", method="lasso"),
#'   rf = find_gene_signature(data, target_var="g3", method="tree_based"),
#'   limma = find_gene_signature(data, target_var="g3", method="limma")
#' )
#'
#' # Train meta-learner on holdout data
#' meta_model <- TML7(
#'   l1_signatures = sigs,
#'   holdout_data = seurat_obj,
#'   target_var = "g3",
#'   l2_methods = c("glm", "ranger", "xgbTree"),  # xgbTree는 성능이 좋지만 xgboost 경고가 나올 수 있음
#'   metric = "AUC",
#'   cv_group_var = "emrid"  # Group-wise CV to prevent patient-level leakage
#' )
#'
#' # Use for prediction
#' predictions <- predict(meta_model$best_model, newdata = new_scores)
#' }
#'
#' @seealso
#'   \code{\link[caret]{train}} for L2 model training,
#'   \code{\link{compute_meta_gene_importance}} for deriving gene-level
#'   importance from the trained meta-learner
#'
#' @export
TML7 <- function(
  l1_signatures,
  holdout_data,
  target_var,
  l2_methods = c("glm","ranger","xgbTree"),
  k_folds   = 5,
  metric    = c("AUC","ROC","Accuracy","Kappa"),
  fgs_seed  = 42,
  layer     = "data",
  allow_parallel = FALSE,
  parallel_workers = NULL,
  cv_group_var = "emrid"
){
  `%||%` <- function(a,b) if (!is.null(a)) a else b

  # ---- NEW: 표준화 유틸 ----
  as_signature <- function(sig){
    # 문자형 유전자 벡터
    if (is.character(sig)) return(structure(rep(1, length(sig)), names = sig))

    # 이름달린 numeric (가중치)
    if (is.numeric(sig) && !is.null(names(sig))) return(sig)

    # list(up=..., down=...)
    if (is.list(sig) && all(c("up","down") %in% names(sig))) {
      up <- sig$up; down <- sig$down
      stopifnot(is.character(up) || is.null(up), is.character(down) || is.null(down))
      up   <- up   %||% character()
      down <- down %||% character()
      nm <- c(up, down)
      wt <- c(rep(1, length(up)), rep(-1, length(down)))
      return(structure(wt, names = nm))
    }

    # list(genes=..., weights=...)
    if (is.list(sig) && all(c("genes","weights") %in% names(sig))) {
      g <- sig$genes; w <- sig$weights
      stopifnot(is.character(g), is.numeric(w), length(g) == length(w))
      names(w) <- g
      return(w)
    }

    # data.frame/tibble: gene/feature + weight/w/score
    if (is.data.frame(sig)) {
      cn <- tolower(colnames(sig))
      gene_col   <- which(cn %in% c("gene","genes","feature","features","symbol","id"))[1]
      weight_col <- which(cn %in% c("weight","weights","w","score","scores","coef","coefs"))[1]
      if (!is.na(gene_col) && !is.na(weight_col)) {
        g <- as.character(sig[[gene_col]])
        w <- as.numeric(sig[[weight_col]])
        ok <- !is.na(g) & !is.na(w)
        w  <- w[ok]; g <- g[ok]
        names(w) <- g
        return(w)
      }
    }

    stop("Unsupported signature format.")
  }

  .score_signature <- function(expr_data, signature, normalize = TRUE) {
    # signature를 표준화해 항상 "이름달린 가중치 numeric"으로 맞춘다
    w <- as_signature(signature)

    genes <- base::intersect(rownames(expr_data), names(w))
    if (length(genes) == 0) stop("Signature has no overlap with expression data.")

    ww <- w[genes]
    # (표준) 가중평균
    s <- as.numeric(Matrix::t(expr_data[genes, , drop = FALSE]) %*% ww)
    den <- sum(abs(ww))
    if (!is.finite(den) || den == 0) den <- length(ww)
    s <- s / den
    
    if (normalize) {
      # zero-variance 체크: scale()이 NA를 반환하는 것을 방지
      s_sd <- stats::sd(s, na.rm = TRUE)
      if (!is.finite(s_sd) || s_sd == 0) {
        # 모든 값이 동일하면 normalize를 건너뛰고 0으로 설정
        s <- rep(0, length(s))
      } else {
        s_scaled <- as.numeric(scale(s))
        # scale()이 NA를 반환할 수 있으므로 체크
        if (any(!is.finite(s_scaled))) {
          warning("Some signature scores became NA/Inf after scaling. Using original scores.")
          # s는 이미 normalize 전 상태이므로 그대로 사용
        } else {
          s <- s_scaled
        }
      }
    }
    s
  }

  .is_binary <- function(y) length(levels(y)) == 2

  metric_choices <- c("AUC", "ROC", "Accuracy", "Kappa")
  if (!is.null(cv_group_var)) {
    cv_group_var <- as.character(cv_group_var)[1]
    if (is.na(cv_group_var)) cv_group_var <- NULL
  }

  .metric_map <- function(user_metric, is_binary) {
    if (is_binary) {
      # caret twoClassSummary → columns: ROC, Sens, Spec
      if (user_metric %in% c("AUC","ROC")) return(list(train_metric="ROC",  summary="twoClassSummary"))
      if (user_metric %in% c("Accuracy","Kappa")) return(list(train_metric=user_metric, summary="twoClassSummary"))
    } else {
      # 멀티클래스 → defaultSummary (Accuracy, Kappa)
      if (user_metric == "AUC") {
        message("WARNING: 'AUC/ROC' not defined for multi-class defaultSummary. Falling back to 'Accuracy'.")
        return(list(train_metric="Accuracy", summary="defaultSummary"))
      }
      return(list(train_metric=user_metric, summary="defaultSummary"))
    }
    list(train_metric = if (is_binary) "ROC" else "Accuracy",
         summary      = if (is_binary) "twoClassSummary" else "defaultSummary")
  }

  .package_ok <- function(pkg){
    suppressWarnings(suppressMessages(requireNamespace(pkg, quietly = TRUE)))
  }

  supported_l2_methods <- c(
    "glm", "ranger", "xgbTree",
    "glmnet", "svmRadial", "mlp",
    "mlpKerasDropout", "nnet", "earth"
  )
  method_dependencies <- c(
    ranger = "ranger",
    xgbTree = "xgboost",
    glmnet = "glmnet",
    svmRadial = "kernlab",
    mlp = "RSNNS",
    mlpKerasDropout = "keras",
    nnet = "nnet",
    earth = "earth"
  )
  default_l2_methods <- c("glm", "ranger", "xgbTree")
  l2_methods <- l2_methods %||% default_l2_methods
  l2_methods <- base::unique(l2_methods)

  unsupported <- base::setdiff(l2_methods, supported_l2_methods)
  if (length(unsupported) > 0) {
    warning(sprintf(
      "Unsupported L2 method(s) removed: %s. Supported methods: %s",
      paste(unsupported, collapse = ", "),
      paste(supported_l2_methods, collapse = ", ")
    ))
    l2_methods <- base::intersect(l2_methods, supported_l2_methods)
  }
  if (length(l2_methods) == 0) {
    stop("No supported L2 methods specified.")
  }

  if (!.package_ok("caret")) stop("caret package required.")
  if (!.package_ok("Matrix")) stop("Matrix package required.")

  register_sequential <- function() {
    if (.package_ok("foreach")) {
      try(foreach::registerDoSEQ(), silent = TRUE)
    }
  }

  # Strict CPU core limiting to prevent cascade parallelization
  # Especially important when child processes load start.R which may spawn more workers
  max_cores_limit <- 16L
  if (requireNamespace("parallel", quietly = TRUE)) {
    detected_cores <- tryCatch(parallel::detectCores(logical = FALSE), error = function(e) NA_integer_)
    if (!is.na(detected_cores)) {
      max_cores_limit <- min(max_cores_limit, as.integer(detected_cores))
    }
  }
  
  # Set environment variables BEFORE any parallel processing
  Sys.setenv(
    OMP_NUM_THREADS = "1",
    OPENBLAS_NUM_THREADS = "1",
    MKL_NUM_THREADS = "1",
    VECLIB_MAXIMUM_THREADS = "1",
    NUMEXPR_NUM_THREADS = "1",
    # Prevent future workers from loading start.R
    KDW_START_LOAD_ALL_PACKAGES = "FALSE",
    KDW_START_AUTOLOAD_QS = "FALSE"
  )
  options(mc.cores = 1L)
  
  allow_parallel <- isTRUE(allow_parallel)
  worker_count <- parallel_workers
  if (is.null(worker_count)) {
    fallback <- getOption("mylit.meta_learner.workers", 4L)
    cores <- tryCatch(parallel::detectCores(logical = FALSE), error = function(e) NA_integer_)
    cores <- cores %||% fallback
    # Limit to max_cores_limit to prevent excessive CPU usage
    worker_count <- max(1L, min(max_cores_limit, as.integer(cores)))
  } else {
    # Also cap user-specified workers
    worker_count <- min(max_cores_limit, as.integer(worker_count))
  }

  # Disable parallel processing by default to prevent cascade effects
  # Even if allow_parallel=TRUE, we'll use sequential execution to avoid
  # child processes spawning more workers via start.R
  allow_parallel <- FALSE
  register_sequential()
  
  # Note: We disable parallel processing to prevent cascade parallelization
  # when child processes load start.R. If parallel processing is needed,
  # it should be handled at a higher level with proper resource management.

  set.seed(fgs_seed)
  RNGkind(sample.kind = "Rejection")

  # --- 데이터 추출 ---
  if (inherits(holdout_data, "Seurat")) {
    if (!.package_ok("Seurat")) stop("Seurat package required for Seurat input.")
    expr_mat <- Seurat::GetAssayData(holdout_data, layer = layer)
    meta_data <- holdout_data@meta.data
    if (!target_var %in% colnames(meta_data))
      stop(sprintf("Target column '%s' not found in Seurat meta.data.", target_var))
    l2_target <- meta_data[[target_var]]
  } else {
    if (is.data.frame(holdout_data)) holdout_data <- as.matrix(holdout_data)
    if (!is.matrix(holdout_data)) stop("holdout_data must be Seurat or a matrix/data.frame [features x cells].")
    expr_mat <- holdout_data
    l2_target <- target_var
  }
  if (is.null(rownames(expr_mat))) stop("Expression matrix must have rownames (feature IDs).")
  if (is.null(colnames(expr_mat))) colnames(expr_mat) <- paste0("cell_", seq_len(ncol(expr_mat)))

  # --- L2 특성 구성 (시그니처 점수 계산) ---
  if (is.null(names(l1_signatures))) names(l1_signatures) <- paste0("L1_", seq_along(l1_signatures))

  l2_features_list <- lapply(l1_signatures, function(sig) {
    tryCatch({
      .score_signature(expr_mat, sig, normalize = TRUE)
    }, error = function(e) {
      warning(sprintf("Failed to score signature '%s': %s", 
                      deparse(substitute(sig))[1], e$message))
      return(rep(NA_real_, ncol(expr_mat)))
    })
  })
  l2_train_df <- as.data.frame(do.call(cbind, l2_features_list))
  colnames(l2_train_df) <- make.names(names(l1_signatures))
  
  # 디버깅: 각 signature의 분산 체크
  sig_variances <- apply(l2_train_df, 2, function(x) {
    x_clean <- x[is.finite(x)]
    if (length(x_clean) == 0) return(NA_real_)
    stats::var(x_clean, na.rm = TRUE)
  })
  zero_var_sigs <- names(sig_variances)[is.na(sig_variances) | sig_variances == 0]
  if (length(zero_var_sigs) > 0 && length(zero_var_sigs) < ncol(l2_train_df)) {
    message(sprintf("Note: %d signature(s) have zero variance before nearZeroVar: %s",
                    length(zero_var_sigs), paste(zero_var_sigs, collapse=", ")))
  }

  # --- 타깃 준비/정리 ---
  if (!is.factor(l2_target)) l2_target <- factor(l2_target)
  orig_lvls <- levels(l2_target)
  safe_lvls <- make.names(orig_lvls)
  if (!all(orig_lvls == safe_lvls)) {
    message(sprintf("Sanitizing target levels: '%s' -> '%s'",
                    paste(orig_lvls, collapse="'/'"),
                    paste(safe_lvls, collapse="'/'")))
    levels(l2_target) <- safe_lvls
  }

  # --- 그룹 컬럼 준비 (Seurat + meta.data 컬럼 있을 때만) ---
  cv_group <- NULL
  if (inherits(holdout_data, "Seurat") && !is.null(cv_group_var) && nzchar(cv_group_var)) {
    meta_data <- holdout_data@meta.data
    if (cv_group_var %in% colnames(meta_data)) {
      cv_group <- as.vector(meta_data[[cv_group_var]])
    } else {
      message(sprintf(
        "cv_group_var '%s' not found in Seurat meta.data; using standard cell-wise CV.",
        cv_group_var
      ))
    }
  } else if (!inherits(holdout_data, "Seurat") && !is.null(cv_group_var) && nzchar(cv_group_var)) {
    message("cv_group_var requires Seurat metadata; using standard cell-wise CV.")
  }

  if (!is.null(cv_group)) {
    if (all(is.na(cv_group))) {
      message(sprintf("cv_group_var '%s' is entirely NA; using standard cell-wise CV.", cv_group_var))
      cv_group <- NULL
    } else if (anyNA(cv_group)) {
      message(sprintf(
        "cv_group_var '%s' contains %d NA value(s); using standard cell-wise CV.",
        cv_group_var, sum(is.na(cv_group))
      ))
      cv_group <- NULL
    }
  }

  keep <- !is.na(l2_target)
  if (!all(keep)) {
    message(sprintf("Removing %d rows with NA target.", sum(!keep)))
    l2_target   <- l2_target[keep]
    l2_train_df <- l2_train_df[keep, , drop = FALSE]
    if (!is.null(cv_group)) cv_group <- cv_group[keep]
  }

  row_ok <- stats::complete.cases(l2_train_df) &
            apply(l2_train_df, 1, function(r) all(is.finite(r)))
  if (!all(row_ok)) {
    message(sprintf("Removing %d rows with NA/NaN/Inf features.", sum(!row_ok)))
    l2_target   <- l2_target[row_ok]
    l2_train_df <- l2_train_df[row_ok, , drop = FALSE]
    if (!is.null(cv_group)) cv_group <- cv_group[row_ok]
  }
  l2_target <- base::droplevels(l2_target)

  nzv <- caret::nearZeroVar(l2_train_df, saveMetrics = FALSE)
  if (length(nzv) > 0) {
    message(sprintf("Removing %d zero-variance features: %s",
                    length(nzv), paste(colnames(l2_train_df)[nzv], collapse=", ")))
    l2_train_df <- l2_train_df[, -nzv, drop = FALSE]
  }

  if (nrow(l2_train_df) == 0 || ncol(l2_train_df) == 0) {
    # 더 자세한 에러 메시지 제공
    n_sigs_input <- length(l1_signatures)
    n_sigs_after_scoring <- length(l2_features_list)
    n_rows_after_na_removal <- sum(row_ok)
    n_cols_after_nzv <- ncol(l2_train_df)
    
    msg <- sprintf(
      "No usable data remains after cleaning.\n  Input signatures: %d\n  Signatures after scoring: %d\n  Rows after NA/Inf removal: %d\n  Columns after zero-variance removal: %d\n  Possible causes: (1) All signature scores are identical (zero variance), (2) All rows contain NA/Inf, (3) Signatures have no overlap with expression data.",
      n_sigs_input, n_sigs_after_scoring, n_rows_after_na_removal, n_cols_after_nzv
    )
    stop(msg)
  }

  # --- metric 매핑 & group-wise CV index 구성 ---
  is_bin <- .is_binary(l2_target)
  metric <- match.arg(metric, metric_choices)
  map <- .metric_map(metric, is_bin)
  caret_metric <- map$train_metric

  summary_fun <- if (map$summary == "twoClassSummary") caret::twoClassSummary else caret::defaultSummary
  positive_class <- if (length(levels(l2_target)) > 0) tail(levels(l2_target), 1) else NA_character_

  index <- indexOut <- NULL
  log_group_fold_details <- function(index_list, index_out_list, group_vector) {
    if (is.null(index_list) || is.null(index_out_list)) return()
    for (fold in seq_along(index_list)) {
      in_idx  <- index_list[[fold]]
      out_idx <- index_out_list[[fold]]
      in_groups <- sort(base::unique(as.character(group_vector[in_idx])))
      out_groups <- sort(base::unique(as.character(group_vector[out_idx])))
      holdout_preview <- if (length(out_groups) > 5) {
        paste0(paste(head(out_groups, 5), collapse = ", "), ", ...")
      } else if (length(out_groups) == 0) {
        "<none>"
      } else {
        paste(out_groups, collapse = ", ")
      }
      message(sprintf(
        "Group CV fold %d: index=%d cells (%d groups), indexOut=%d cells (%d groups), hold-out groups: %s",
        fold,
        length(in_idx),
        length(in_groups),
        length(out_idx),
        length(out_groups),
        holdout_preview
      ))
    }
  }
  use_group_cv <- !is.null(cv_group) && !all(is.na(cv_group))

  if (use_group_cv) {
    group_factor  <- factor(cv_group)
    unique_groups <- levels(group_factor)

    if (length(unique_groups) < k_folds) {
      message(sprintf(
        "cv_group_var '%s' has only %d unique values (< k_folds=%d); using standard cell-wise CV.",
        cv_group_var, length(unique_groups), k_folds
      ))
      use_group_cv <- FALSE
    } else {
      set.seed(fgs_seed)
      fold_assign <- sample(rep(seq_len(k_folds), length.out = length(unique_groups)))
      names(fold_assign) <- unique_groups

      index    <- vector("list", k_folds)
      indexOut <- vector("list", k_folds)

      for (fold in seq_len(k_folds)) {
        in_groups  <- unique_groups[fold_assign != fold]
        out_groups <- unique_groups[fold_assign == fold]
        in_idx  <- which(group_factor %in% in_groups)
        out_idx <- which(group_factor %in% out_groups)
        index[[fold]]    <- in_idx
        indexOut[[fold]] <- out_idx
      }

      message(sprintf(
        "Using group-wise CV on '%s' (%d groups) for L2 (k_folds=%d).",
        cv_group_var, length(unique_groups), k_folds
      ))
      log_group_fold_details(index, indexOut, group_factor)
    }
  }
  if (!use_group_cv) {
    message("Using standard cell-wise CV folds (caret-generated).")
  }

  # Always disable parallel processing in caret to prevent cascade parallelization
  # Child processes from future/doParallel may load start.R and spawn more workers
  ctrl <- caret::trainControl(
    method = "cv", number = k_folds,
    classProbs = is_bin,
    summaryFunction = summary_fun,
    savePredictions = "final",
    allowParallel = FALSE,  # Always FALSE to prevent cascade parallelization
    index    = index,
    indexOut = indexOut
  )

  drop_if_missing <- function(method_name, pkg) {
    if (method_name %in% l2_methods && !.package_ok(pkg)) {
      warning(sprintf("%s package required for '%s'; dropping.", pkg, method_name))
      l2_methods <<- base::setdiff(l2_methods, method_name)
    }
  }

  deps_to_check <- base::intersect(names(method_dependencies), l2_methods)
  if (length(deps_to_check) > 0) {
    for (m in deps_to_check) {
      drop_if_missing(m, method_dependencies[[m]])
    }
  }

  if ("xgbTree" %in% l2_methods) {
    if (!.package_ok("xgboost")) {
      warning("xgboost not available; dropping 'xgbTree'.")
      l2_methods <- base::setdiff(l2_methods, "xgbTree")
    } else {
      ok <- TRUE
      tryCatch(utils::packageVersion("xgboost"), error = function(e) ok <<- FALSE)
      if (!ok) {
        warning("xgboost seems broken; dropping 'xgbTree'.")
        l2_methods <- base::setdiff(l2_methods, "xgbTree")
      } else {
        # 가능하면 전역 verbosity를 0으로 낮춰 C-level 경고(특히 ntree_limit)를 숨긴다
        if (isTRUE(.package_ok("xgboost"))) {
          xgb_ns <- asNamespace("xgboost")
          if (exists("xgb.set.config", envir = xgb_ns, mode = "function")) {
            try(xgboost::xgb.set.config(verbosity = 0), silent = TRUE)
          }
        }
      }
    }
  }
  if (length(l2_methods) == 0) stop("No usable models remain.")

  message(sprintf(
    "Validated L2 method set (%d): %s",
    length(l2_methods),
    paste(l2_methods, collapse = ", ")
  ))

  # Ensure no parallel backends are active before training
  if (requireNamespace("foreach", quietly = TRUE)) {
    try(foreach::registerDoSEQ(), silent = TRUE)
  }
  if (requireNamespace("doParallel", quietly = TRUE)) {
    try(doParallel::stopImplicitCluster(), silent = TRUE)
  }
  if (requireNamespace("doMC", quietly = TRUE)) {
    try(doMC::registerDoMC(cores = 1), silent = TRUE)
  }
  
  model_list <- list()
  for (m in l2_methods) {
    message(sprintf("Training L2 candidate: %s (metric=%s)", m, caret_metric))
    
    # Method-specific parallel disabling
    if (m == "xgbTree" && requireNamespace("xgboost", quietly = TRUE)) {
      # xgboost nthread 설정
      xgb_params <- list(nthread = 1)
    } else if (m == "ranger" && requireNamespace("ranger", quietly = TRUE)) {
      # ranger num.threads 설정
      ranger_params <- list(num.threads = 1)
    } else {
      xgb_params <- NULL
      ranger_params <- NULL
    }
    
    if (m == "xgbTree") {
      # R-level warning은 suppressWarnings로, C-level 로그는 위에서 xgb.set.config(verbosity=0)로 최대한 억제
      fit <- try(
        suppressWarnings(
          caret::train(
            x = l2_train_df,
            y = l2_target,
            method = m,
            trControl = ctrl,
            metric = caret_metric,
            tuneLength = 5,
            nthread = 1  # Force single-threaded xgboost
          )
        ), silent = TRUE
      )
    } else if (m == "ranger") {
      fit <- try(
        caret::train(
          x = l2_train_df,
          y = l2_target,
          method = m,
          trControl = ctrl,
          metric = caret_metric,
          tuneLength = 5,
          num.threads = 1  # Force single-threaded ranger
        ), silent = TRUE
      )
    } else {
      fit <- try(
        caret::train(
          x = l2_train_df,
          y = l2_target,
          method = m,
          trControl = ctrl,
          metric = caret_metric,
          tuneLength = 5
        ), silent = TRUE
      )
    }
    if (inherits(fit, "try-error")) {
      warning(sprintf("Failed to train '%s': %s", m, as.character(fit)))
    } else {
      model_list[[m]] <- fit
    }
  }
  if (length(model_list) == 0) stop("No L2 models were successfully trained.")

  if (length(model_list) == 1) {
    best_name <- names(model_list)[1]
    best_fit  <- model_list[[1]]
    best_val  <- suppressWarnings(max(best_fit$results[[caret_metric]], na.rm = TRUE))
    message(sprintf("Only one model trained. Selected '%s' (CV %s=%.4f).",
                    best_name, caret_metric, best_val))
    resamp <- NULL
  } else {
    resamp <- caret::resamples(model_list)
    vals <- sapply(model_list, function(f) suppressWarnings(max(f$results[[caret_metric]], na.rm = TRUE)))
    best_name <- names(which.max(vals))
    best_fit  <- model_list[[best_name]]
    message(sprintf("Best model: %s (CV %s=%.4f).", best_name, caret_metric, max(vals, na.rm = TRUE)))
  }

  list(
    best_model       = best_fit,
    best_model_name  = best_name,
    best_metric_name = caret_metric,
    model_comparison = resamp,
     trained_models   = model_list,
     positive_class   = positive_class,
    l2_train         = data.frame(l2_train_df, .target = l2_target),
    l1_signatures    = l1_signatures
  )
}



#' Derive gene-level importance from a trained meta learner
#'
#' Combines L2 signature importances with L1 signature weights to obtain
#' per-gene contribution scores. When the selected meta learner is a logistic
#' regression, signed contributions indicate whether higher gene expression
#' increases (`> 0`) or decreases (`< 0`) the odds of the positive class.
#' For non-linear models the magnitude is still meaningful, but signs should be
#' interpreted cautiously because the L2 model can capture non-linear effects.
#'
#' @param meta_result A result list returned by [TML7()].
#' @param normalize If `TRUE`, scale contributions so that the maximum absolute
#'   per-signature contribution is 1.
#' @return A list with elements:
#'   \describe{
#'     \item{signature_importance}{Named numeric vector of L1 signature importances.}
#'     \item{gene_importance}{Data frame with columns `signature`, `gene`,
#'           `contribution` (signed), and `abs_contribution`.}
#'     \item{gene_summary}{Aggregated contributions per gene across signatures.}
#'     \item{positive_class}{Name of the positive class (usually `X2`).}
#'     \item{model_type}{Name of the selected L2 model.}
#'   }
#' @export
compute_meta_gene_importance <- function(meta_result, normalize = TRUE) {
  if (!is.list(meta_result) ||
      !all(c("best_model", "best_model_name", "l1_signatures", "l2_train") %in% names(meta_result))) {
    stop("meta_result must be the object returned by TML7().")
  }

  `%||%` <- function(x, y) if (!is.null(x)) x else y

  model_type <- meta_result$best_model_name
  model <- meta_result$best_model
  sig_names <- base::setdiff(colnames(meta_result$l2_train), ".target")

  if (length(sig_names) == 0) {
    stop("No signature features found in l2_train.")
  }

  # helper to standardise signature definition into named numeric vector
  standardise_signature <- function(sig) {
    if (is.null(sig)) return(numeric(0))
    if (is.numeric(sig) && !is.null(names(sig))) return(sig)
    if (is.character(sig)) return(structure(rep(1, length(sig)), names = sig))
    if (is.list(sig) && all(c("up", "down") %in% names(sig))) {
      up <- sig$up %||% character()
      down <- sig$down %||% character()
      nm <- c(up, down)
      wt <- c(rep(1, length(up)), rep(-1, length(down)))
      names(wt) <- nm
      return(wt)
    }
    if (is.list(sig) && all(c("genes", "weights") %in% names(sig))) {
      g <- sig$genes
      w <- sig$weights
      if (length(g) != length(w)) stop("Signature genes/weights length mismatch.")
      names(w) <- g
      return(w)
    }
    if (is.data.frame(sig)) {
      cn <- tolower(colnames(sig))
      gene_col <- which(cn %in% c("gene","genes","feature","features","symbol","id"))[1]
      weight_col <- which(cn %in% c("weight","weights","w","score","scores","coef","coefs"))[1]
      if (!is.na(gene_col) && !is.na(weight_col)) {
        g <- as.character(sig[[gene_col]])
        w <- as.numeric(sig[[weight_col]])
        ok <- !is.na(g) & !is.na(w)
        w <- w[ok]; g <- g[ok]
        names(w) <- g
        return(w)
      }
    }
    stop("Unsupported signature format when standardising weights.")
  }

  signature_weights <- lapply(meta_result$l1_signatures[sig_names], standardise_signature)

  # obtain signature-level importance
  signature_importance <- NULL
  if (identical(model_type, "glm") && inherits(model$finalModel, "glm")) {
    coefs <- stats::coef(model$finalModel)
    coefs <- coefs[names(coefs) != "(Intercept)"]
    # sig_names와 일치하는 것만 추출
    available_sigs <- base::intersect(sig_names, names(coefs))
    if (length(available_sigs) == 0) {
      stop("No matching signatures found in glm coefficients.")
    }
    signature_importance <- coefs[available_sigs]
    names(signature_importance) <- available_sigs
  } else {
    vi <- try(caret::varImp(model, scale = FALSE), silent = TRUE)
    if (inherits(vi, "try-error") || is.null(vi)) {
      stop(sprintf("caret::varImp() failed for model type '%s'. Error: %s",
                   model_type, as.character(vi)), call. = FALSE)
    }
    importance_df <- vi$importance
    if (is.null(importance_df) || !is.data.frame(importance_df)) {
      stop(sprintf("varImp for model '%s' did not produce a data.frame.", model_type), call. = FALSE)
    }
    # rownames 확인 및 매칭
    imp_rownames <- rownames(importance_df)
    available_sigs <- base::intersect(sig_names, imp_rownames)
    if (length(available_sigs) == 0) {
      stop("No matching signatures found in varImp results. Expected: ", 
           paste(sig_names, collapse=", "), 
           "; Found: ", paste(imp_rownames, collapse=", "))
    }
    
    if ("Overall" %in% colnames(importance_df)) {
      signature_importance <- importance_df[available_sigs, "Overall", drop = TRUE]
    } else if (ncol(importance_df) >= 1) {
      signature_importance <- importance_df[available_sigs, 1, drop = TRUE]
    }
    # named vector로 변환
    if (is.null(names(signature_importance))) {
      names(signature_importance) <- available_sigs
    }
  }

  if (is.null(signature_importance) || length(signature_importance) == 0) {
    stop("Could not derive signature importance for model type: ", model_type)
  }

  # 사용 가능한 signature만 필터링
  available_sigs <- names(signature_importance)
  missing_sigs <- base::setdiff(sig_names, available_sigs)
  if (length(missing_sigs) > 0) {
    warning(sprintf("Some signatures not found in importance: %s. They will be skipped.",
                    paste(missing_sigs, collapse=", ")))
  }

  if (normalize) {
    signature_importance <- signature_importance / (max(abs(signature_importance), na.rm = TRUE) %||% 1)
  }

  gene_tables <- lapply(available_sigs, function(sig) {
    weights <- signature_weights[[sig]]
    if (length(weights) == 0) return(NULL)
    # named vector이므로 [ 사용 ([[ 아님)
    sig_imp <- signature_importance[sig]
    if (is.na(sig_imp) || !is.finite(sig_imp)) {
      warning(sprintf("Signature '%s' has invalid importance value. Skipping.", sig))
      return(NULL)
    }
    contrib <- sig_imp * weights
    if (normalize && any(is.finite(contrib))) {
      denom <- max(abs(contrib), na.rm = TRUE)
      if (is.finite(denom) && denom > 0) contrib <- contrib / denom
    }
    data.frame(
      signature = sig,
      gene = names(contrib),
      contribution = as.numeric(contrib),
      abs_contribution = abs(as.numeric(contrib)),
      stringsAsFactors = FALSE
    )
  })

  gene_importance <- do.call(rbind, gene_tables)
  if (is.null(gene_importance) || nrow(gene_importance) == 0) {
    stop("No gene contributions could be computed.")
  }

  gene_summary <- stats::aggregate(contribution ~ gene, data = gene_importance, sum)
  gene_summary$abs_contribution <- abs(gene_summary$contribution)

  positive_class <- meta_result$positive_class %||%
    tail(levels(meta_result$l2_train$.target), 1)

  list(
    signature_importance = signature_importance,
    gene_importance = gene_importance,
    gene_summary = gene_summary[order(-gene_summary$abs_contribution), ],
    positive_class = positive_class,
    model_type = model_type
  )
}

#' Add Meta-Learner Signature Score to Seurat Object
#'
#' @description
#'   Computes signature scores using the meta-learner gene weights from
#'   `compute_meta_gene_importance()` and adds them to a Seurat object's metadata.
#'   The signature score is calculated as a weighted sum of gene expression values,
#'   where weights are derived from the meta-learner's gene-level importance.
#'
#' @param seurat_obj A Seurat object containing expression data.
#' @param gene_importance_result Result from `compute_meta_gene_importance()`.
#'   Must contain `gene_summary` with columns `gene` and `contribution`.
#' @param signature_name Name for the metadata column (default: "meta_signature_score").
#' @param assay Assay name to use (default: DefaultAssay(seurat_obj)).
#' @param layer Layer to pull expression data from (default: "data").
#' @param normalize Whether to z-score normalize the signature scores (default: TRUE).
#' @param use_abs_contribution Whether to use absolute contribution values as weights
#'   (default: FALSE, uses signed contribution).
#'
#' @return A Seurat object with signature scores added to metadata.
#'
#' @details
#'   The signature score for each cell/AOI is calculated as:
#'   \deqn{s_c = \frac{\sum_g w_g \cdot x_{gc}}{\sum_g |w_g|}}
#'   where \eqn{w_g} are the gene weights (contribution values) and \eqn{x_{gc}} are
#'   the expression values for gene \eqn{g} and cell \eqn{c}.
#'
#'   If `normalize=TRUE`, scores are further z-scored:
#'   \deqn{s_c' = \frac{s_c - \mu_s}{\sigma_s}}
#'
#' @examples
#' \dontrun{
#' # After running TML7 and compute_meta_gene_importance
#' tml_result <- TML7(l1_signatures, data_seurat, "response", cv_group_var = "emrid")
#' cmgi_result <- compute_meta_gene_importance(tml_result)
#'
#' # Add signature scores to Seurat object
#' data_seurat <- add_meta_signature_score(
#'   seurat_obj = data_seurat,
#'   gene_importance_result = cmgi_result,
#'   signature_name = "response_signature"
#' )
#'
#' # Visualize
#' FeaturePlot(data_seurat, features = "response_signature")
#' }
#'
#' @export
add_meta_signature_score <- function(
  seurat_obj,
  gene_importance_result,
  signature_name = "meta_signature_score",
  assay = NULL,
  layer = "data",
  normalize = TRUE,
  use_abs_contribution = FALSE
) {
  if (!inherits(seurat_obj, "Seurat")) {
    stop("seurat_obj must be a Seurat object.")
  }

  if (!is.list(gene_importance_result) ||
      !"gene_summary" %in% names(gene_importance_result)) {
    stop("gene_importance_result must be the output of compute_meta_gene_importance().")
  }

  gene_summary <- gene_importance_result$gene_summary
  if (!all(c("gene", "contribution") %in% colnames(gene_summary))) {
    stop("gene_summary must contain 'gene' and 'contribution' columns.")
  }

  # Get assay
  if (is.null(assay)) {
    assay <- Seurat::DefaultAssay(seurat_obj)
  }

  # Get expression matrix
  expr_mat <- Seurat::GetAssayData(seurat_obj, assay = assay, layer = layer)
  if (is.null(expr_mat)) {
    stop(sprintf("Layer '%s' not found in assay '%s'.", layer, assay))
  }

  # Prepare gene weights
  if (use_abs_contribution) {
    if (!"abs_contribution" %in% colnames(gene_summary)) {
      gene_summary$abs_contribution <- abs(gene_summary$contribution)
    }
    weights <- gene_summary$abs_contribution
  } else {
    weights <- gene_summary$contribution
  }
  names(weights) <- gene_summary$gene

  # Find overlapping genes
  available_genes <- base::intersect(names(weights), rownames(expr_mat))
  if (length(available_genes) == 0) {
    stop("No overlapping genes found between signature and expression data.")
  }
  if (length(available_genes) < length(weights)) {
    warning(sprintf("Only %d/%d signature genes found in expression data.",
                    length(available_genes), length(weights)))
  }

  # Subset weights to available genes
  w <- weights[available_genes]

  # Calculate signature scores
  # Weighted sum: sum(w * expr) / sum(|w|)
  expr_subset <- expr_mat[available_genes, , drop = FALSE]
  if (inherits(expr_subset, "sparseMatrix")) {
    # For sparse matrices, use Matrix operations
    scores <- as.numeric(Matrix::t(expr_subset) %*% w)
  } else {
    # For dense matrices
    scores <- as.numeric(colSums(expr_subset * w))
  }

  # Normalize by sum of absolute weights
  den <- sum(abs(w))
  if (!is.finite(den) || den == 0) {
    warning("Sum of absolute weights is zero or non-finite. Using unnormalized scores.")
    den <- 1
  }
  scores <- scores / den

  # Z-score normalization if requested
  if (normalize) {
    scores_sd <- stats::sd(scores, na.rm = TRUE)
    if (is.finite(scores_sd) && scores_sd > 0) {
      scores <- as.numeric(scale(scores))
    } else {
      warning("Cannot normalize scores (zero variance). Using unnormalized scores.")
    }
  }

  # Add to metadata
  seurat_obj <- Seurat::AddMetaData(
    object = seurat_obj,
    metadata = scores,
    col.name = signature_name
  )

  message(sprintf("Added signature score '%s' to metadata (%d genes used).",
                  signature_name, length(available_genes)))

  return(seurat_obj)
}

